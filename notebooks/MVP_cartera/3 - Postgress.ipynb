{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "from pyspark.sql import functions as F, DataFrame\n",
    "import datetime as dt\n",
    "from datetime import date, datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.types as t\n",
    "from decimal import Decimal\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataproc_sdk.dataproc_sdk_datiopysparksession.datiopysparksession import DatioPysparkSession\n",
    "datioSparkSession = DatioPysparkSession().get_or_create()\n",
    "\n",
    "from dataproc_sdk.dataproc_sdk_datiopysparksession import datiopysparksession\n",
    "dataproc = datiopysparksession.DatioPysparkSession().get_or_create()\n",
    "\n",
    "from dataproc_sdk.dataproc_sdk_schema.datioschema import DatioSchema\n",
    "from dataproc_sdk.dataproc_sdk_datiofilesystem.datiofilesystem import DatioFileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para evitar problemas de tipología de datos\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hacer que la función no tome los csv para calcular la fecha, que se quede solo en el primer nivel de closing date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculamos la fecha más reciente de la ruta tomando como campo de particion el pasado como parámetro\n",
    "# def last_partition (path:str, campo:str):\n",
    "#     window = Window.partitionBy().orderBy(F.col(campo).desc())\n",
    "    \n",
    "#     # opcion 1\n",
    "#     df = dataproc.read().parquet(path\n",
    "#                                ).withColumn(\"rn\", F.row_number().over(window)\n",
    "#                                ).where(F.col(\"rn\") == 1).drop('rn')\n",
    "    \n",
    "#     # opcion 2\n",
    "#     # df = dataproc.read().parquet(path).coalesce(1\n",
    "#     #                                            ).orderBy(F.col(campo).asc()\n",
    "#     #                                            ).groupBy().agg(F.last(campo).alias(campo))\n",
    "    \n",
    "#     fecha = [x[campo] for x in df.select(campo).collect()][0]\n",
    "#     print('Fecha de datos:',fecha)\n",
    "#     return fecha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos la fecha más reciente de la ruta tomando como campo de particion el pasado como parámetro\n",
    "def last_partition (p_path:str, campo:str):\n",
    "    \n",
    "    datio_path = DatioFileSystem().get().qualify(p_path)\n",
    "    fs = datio_path.fileSystem()\n",
    "    path = datio_path.path()\n",
    "    path_list = fs.listStatus(path)\n",
    "    paths = [path.getPath().toString() for path in path_list] #listado de todos los paths de la ruta pasada\n",
    "    \n",
    "    l_fechas = [element.split(campo+'=')[1] for element in paths if campo in element] #listado de todas las fechas\n",
    "    return max(l_fechas) # fecha mayor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta Raiz en SBX\n",
    "root_path = '/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/cartera_optima/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_write= 'overwrite'# 'append'\n",
    "# f_write = '2024-06-18' # particion de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_write = last_partition (root_path, 'closing_date')\n",
    "root_pathc = root_path + 'closing_date=' + str(f_write)\n",
    "root_pathc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas datos en SBX\n",
    "path_facilities = root_pathc + '/facilities'\n",
    "\n",
    "path_limites_only = root_pathc + '/limites'\n",
    "\n",
    "path_constantes = root_pathc + '/constants'\n",
    "\n",
    "path_excluidas = root_pathc + '/facilities_excluded'\n",
    "\n",
    "path_facilities_total = root_pathc + '/cartera_titulizar' # foto con toda la traza de las operaciones, para poder seleccionar las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = 'joystick'\n",
    "\n",
    "TABLES = {'joystick.securitization_limit':path_limites_only,\n",
    "         'joystick.securitization_constant':path_constantes,\n",
    "         'joystick.securitization_facilities_excluded':path_excluidas,\n",
    "         'joystick.securitization_facilities_completed':path_facilities_total,\n",
    "         # 'joystick.securitization_facilities:\n",
    "         } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDICES = {'i_piloto': ['BEC_CDM_RELACIONES_POSTG','BEC_CDM_RELACIONES_CAPT_POSTG',\n",
    "#                         'BEC_CDM_PILOTOS_POSTG','BEC_CDM_PILOTOS_CAPT_POSTG'],\n",
    "#           'i_relacionada': ['BEC_CDM_RELACIONES_POSTG','BEC_CDM_RELACIONES_CAPT_POSTG',\n",
    "#                             'BEC_CDM_RELACIONADAS_POSTG','BEC_CDM_RELACIONADAS_CAPT_POSTG'],\n",
    "#           'i_oficina': ['BEC_CDM_PILOTOS_POSTG','BEC_CDM_PILOTOS_CAPT_POSTG'],\n",
    "#           'i_gestor': ['BEC_CDM_PILOTOS_POSTG','BEC_CDM_PILOTOS_CAPT_POSTG'],}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.getenv(\"JPY_USER\")\n",
    "jpy = os.getenv(\"JPY_BASE_URL\")\n",
    "uuaa = jpy.split(\"/\")[2]\n",
    "vault = os.getenv(\"VAULT_HOSTS\")\n",
    "environ = vault.split(\".\")[3]\n",
    "country = vault.split(\".\")[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('user:',user)\n",
    "print('jpy:',jpy)\n",
    "print('uuaa:',uuaa)\n",
    "print('vault:',vault)\n",
    "print('environ:',environ)\n",
    "print('country:',country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here the postgres host, port and database                        \n",
    "#postgres_host = \"ep-w-pgreporting.marathon.l4lb.\" + environ + \".daas.\" + country + \".igrupobbva:81\"\n",
    "postgres_host = \"ep-w-pgreporting.sandbox.extservices.platform.marathon.mesos:5432\"\n",
    "postgres_db = uuaa\n",
    "print(postgres_host + \"/\" + postgres_db)\n",
    "\n",
    "# Path to needed certificates taken from Spark properties\n",
    "sslKey = \"/opt/sds/stratio_analytic/security/\" + user + \".key\"\n",
    "sslCert = \"/opt/sds/stratio_analytic/security/\"  + user + \".pem\"\n",
    "sslRootCert = \"/opt/sds/stratio_analytic/security/ca-bundle.pem\"\n",
    "\n",
    "pgurl = (\"postgresql://{user}@{host}/{db}?sslmode=verify-full&sslcert={sslCert}&sslrootcert={sslRootCert}&sslkey={sslKey}\"\n",
    "               .format(sslCert=sslCert,\n",
    "                       sslKey=sslKey,\n",
    "                       sslRootCert=sslRootCert,\n",
    "                       user=user,\n",
    "                       host=postgres_host,\n",
    "                       db=postgres_db))\n",
    "pg = create_engine(pgurl)\n",
    "\n",
    "\n",
    "sslCertdbc = spark.conf.get(\"spark.ssl.datastore.certPem.path\")\n",
    "sslKeydbc = spark.conf.get(\"spark.ssl.datastore.keyPKCS8.path\")\n",
    "sslRootCertdbc = spark.conf.get(\"spark.ssl.datastore.caPem.path\")\n",
    "\n",
    "pgjdbc = (\"jdbc:postgresql://{host}/{db}?user={user}&ssl=true&sslmode=verify-full&sslcert={sslCert}&sslrootcert={sslRootCert}&sslkey={sslKey}&prepareThreshold=0\"\n",
    "               .format(sslCert=sslCertdbc,\n",
    "                       sslKey=sslKeydbc,\n",
    "                       sslRootCert=sslRootCertdbc,\n",
    "                       user=user,\n",
    "                       host=postgres_host,\n",
    "                       db=postgres_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borramos Tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in TABLES.items():\n",
    "    try:\n",
    "        pg.execute(\"drop table \"+k)\n",
    "        print('Tabla borrada:',k)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos el esquema donde se van a almacenar las tablas\n",
    "try:\n",
    "    pg.execute('CREATE SCHEMA ' + SCHEMA)\n",
    "    print('Esquema generado:',SCHEMA)\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generamos Tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in TABLES.items():\n",
    "    try:\n",
    "        print('**** TABLA: ',k,'*****')\n",
    "        print('**** path de datos: ',v,'*****')\n",
    "        # leemos dataframe\n",
    "        df = spark.read.parquet(v)\n",
    "#         df.show(5,False)\n",
    "        df.write.option(\"truncate\",\"true\").jdbc(url=pgjdbc,table=k,mode=mode_write)        \n",
    "        print('numero de registros:',df.count())\n",
    "\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Añadir Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in INDICES.keys():\n",
    "#     pg.execute('DROP INDEX '+ i)\n",
    "#     print('indice borrado ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in TABLES.items():\n",
    "#     try:\n",
    "#         print('tabla:',k)\n",
    "# #         if(k in INDICES['i_piloto']):\n",
    "# #             pg.execute('CREATE INDEX i_piloto ON ' + k + '(cif_piloto)')\n",
    "# #             print('indice i_piloto generado en tabla ',k)\n",
    "            \n",
    "# #         if(k in INDICES['i_relacionada']):\n",
    "# #             pg.execute('CREATE INDEX i_relacionada ON ' + k + '(cif_relacionada)')\n",
    "# #             print('indice i_relacionada generado en tabla ',k)\n",
    "            \n",
    "#         if(k in INDICES['i_oficina']):\n",
    "#             pg.execute('CREATE INDEX i_oficina ON ' + k + '(oficina_desc_piloto)')\n",
    "#             print('indice i_oficina generado en tabla ',k)\n",
    "            \n",
    "#         if(k in INDICES['i_gestor']):\n",
    "#             pg.execute('CREATE INDEX i_gestor ON ' + k + '(gestor_piloto)')\n",
    "#             print('indice i_gestor generado en tabla ',k)\n",
    "#     except Exception:\n",
    "#         print('Se ha producido un error')\n",
    "#         pass\n",
    "# #     except:\n",
    "# #         print('Se ha producido un error')\n",
    "# #         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(spark.read.parquet(path_pilotos_postg_cap).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"SELECT * FROM joystick.concentration_risk_commitment_P WHERE group_id='G00000000001233'\"\n",
    "query = \"SELECT * FROM joystick.securitization_limit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = pg.execute(query)\n",
    "pt.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todas las filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_as_dicts(cursor):\n",
    "    \"\"\"convert tuple result to dict with cursor\"\"\"\n",
    "    col_names = [i[0] for i in cursor.description]\n",
    "    return [dict(zip(col_names, row)) for row in cursor]\n",
    "\n",
    "cursor = pg.execute(query).cursor\n",
    "result = rows_as_dicts(cursor)\n",
    "\n",
    "for k in result:\n",
    "    print(k)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tablas origen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/cartera_optima/closing_date='+f_write+'/limites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-python",
   "language": "python",
   "name": "spark-python-spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
