{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce19fbd7-1a61-4d6b-985f-2ce7986fa922",
   "metadata": {},
   "source": [
    "**Seleccionamos el grupo de pr√©stamos m√°s √≥ptimo a titulizar seg√∫n los l√≠mites establecidos**\n",
    "- Por defecto lo que no venga marcado como l√≠mite es suceptible de cogerse para la titulizaci√≥n con un 100% (limit value por defecto 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13df4f55-8087-4c84-bdc6-53a073cbf78a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "from pyspark.sql import functions as F, DataFrame\n",
    "import datetime as dt\n",
    "from datetime import date, datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.types as t\n",
    "from decimal import Decimal\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcba62c0-595d-4c42-b7fa-4b75626d86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataproc_sdk.dataproc_sdk_datiopysparksession.datiopysparksession import DatioPysparkSession\n",
    "datioSparkSession = DatioPysparkSession().get_or_create()\n",
    "\n",
    "from dataproc_sdk.dataproc_sdk_datiopysparksession import datiopysparksession\n",
    "dataproc = datiopysparksession.DatioPysparkSession().get_or_create()\n",
    "\n",
    "from dataproc_sdk.dataproc_sdk_schema.datioschema import DatioSchema\n",
    "from dataproc_sdk.dataproc_sdk_datiofilesystem.datiofilesystem import DatioFileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f77a31e-cb82-4521-a60f-9d5c7235e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para evitar problemas de tipolog√≠a de datos\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "# para evitar problemas de particiones\n",
    "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067ae42a-c449-4335-b0da-ce780e41d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# procesamiento de la cartera √≥ptima en python\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39465c57-df80-4bbe-874f-150c7b4c6a27",
   "metadata": {},
   "source": [
    "# Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7857b4c6-d533-4161-acca-76179aaaed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha = date.today() # por defecto la fecha de hoy (se actualizar√° en el proceso con la m√°s reciente)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10040943-8068-497f-8d81-8bc13ee09e8a",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58052bd-6144-4123-8fac-0fab74a90721",
   "metadata": {},
   "source": [
    "### Paths in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14de790c-11ae-4ee9-b0bd-bc2e894ea009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/cartera_optima/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = '/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/cartera_optima/'\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0defbc26-9e4d-47cd-8e47-2ef0a1884af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relaci√≥n entre l√≠mites y a qu√© campo de Datio aplica\n",
    "path_campos = '/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/limites/campos_datio/current/limites_camposDatio.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54145a-15f3-4478-a3fb-a68a874f0a9b",
   "metadata": {},
   "source": [
    "### Paths out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e34c4f0-6bdd-46f0-abba-074392c8686d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/cartera_optima/closing_date=2024-10-22/cartera_titulizar'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_out = root_path+'closing_date='+str(fecha)+'/cartera_titulizar'\n",
    "path_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "493a528f-9b51-4455-9c9b-2b13a6dbba5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/cartera_optima/closing_date=2024-10-22/cartera_titulizar_csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_out_csv = root_path+'closing_date='+str(fecha)+'/cartera_titulizar_csv'\n",
    "path_out_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea05132-cac1-4a5b-80e3-3a70cf86abea",
   "metadata": {},
   "source": [
    "## Columnas\n",
    "Selecci√≥n de columnas necesarias para acotar los limites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dfaee2a-2ef7-4383-b877-c4c58c11d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clave unica de operaci√≥n\n",
    "key_facility = ['delta_file_id','delta_file_band_id','branch_id']\n",
    "key_facility_micro = ['delta_file_id','delta_file_band_id'] # en MicroStrategy se quedan a este nivel\n",
    "\n",
    "# clave del launchpad limites\n",
    "key_limites = ['limit_escenario', 'limit_fecha', 'limit_portfolio_size']\n",
    "\n",
    "# # columnas directas de la salida titulizaciones de joystick\n",
    "# cols_lim = ['gf_ma_ead_amount','Total_Amount_EUR', # exposicion a default y consumo de capital\n",
    "#             'clan_date','com_product', # fecha de los datos y tipo de producto(project finance o corporate loan)\n",
    "#             'gf_pf_current_ratg_id','group_rating_sp', # an√°lisis de rating individual para project finace y corporate loan\n",
    "#             'project_country_desc','customer_country', # pais donde se lleva el proyecto y pais del cliente\n",
    "#             'project_sector_desc','project_subsector_desc', # sector y subsector del proyecto\n",
    "#             'g_asset_allocation_sector_desc','g_asset_allocation_subsec_desc', # sector y subsector de la actividad del cliente\n",
    "#             'customer_country','group_country_desc',\n",
    "#             'financial_product_desc','currency_id']\n",
    "#             #'gf_pf_project_const_type',\n",
    "\n",
    "# # columnas nuevas calculadas \n",
    "# cols_lim_c = ['non_ig_flag', # rating malos\n",
    "#               'building_project_flag', # proyecto en construcci√≥n\n",
    "#              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1785f0-cb9d-460f-8bdd-db0cd69e3ec7",
   "metadata": {},
   "source": [
    "## Diccionarios\n",
    "Se genera un diccionario para el grupo de facilities que entran en la cartera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95da54-0bd3-489e-b082-f5585603e56d",
   "metadata": {},
   "source": [
    "### Tipos de Facility\n",
    "tipolog√≠a de las operaciones que entran en la titulizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d9e4adf-bd09-4bb5-afb6-5028309d4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tipo_facility = {'corporate_loan':'Corporate Facilities', # titulizacion tipo Corporate solo operaciones corporate\n",
    "                 'project_finance':'Project Finance'} # titulizacion tipo Project solo operaciones project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96376f0-a316-47ae-8773-4035376707ae",
   "metadata": {},
   "source": [
    "### Limitaci√≥n importe\n",
    "sobre que campo aplica el % de los l√≠mites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "934ed278-5d97-467b-84e8-4b9ce0311b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_imp_limit = {'individual':'importe_susceptible', # el % del limite se aplica sobre el importe susceptible de la facility\n",
    "               'portfolio':'portfolio_size'} # el % del limite se aplica sobre el importe marcado en el portfolio size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18f19e-5b6e-402a-b934-c2c305309e93",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cef53e-45d3-4325-97dd-b71c92660ed4",
   "metadata": {},
   "source": [
    "### Porcentajes\n",
    "para aquellos limites que su % va ligado a alg√∫n c√°lculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1507e062-0b64-4743-a079-67845f681fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_limit_value (df:DataFrame):\n",
    "    df_l = df.withColumn('limit_value', \n",
    "                         F.when(F.col('limit_type')=='risk_retention',(F.lit(1) - F.col('limit_value')).cast('float')\n",
    "                               ).otherwise(F.col('limit_value')))\n",
    "    return df_l       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e29a1-5deb-4d8f-9828-565930e7dbee",
   "metadata": {},
   "source": [
    "### Gen√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59bd51f9-bff6-41c2-8186-eae723b12db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos la fecha m√°s reciente de la ruta tomando como campo de particion el pasado como par√°metro\n",
    "def last_partition (p_path:str, campo:str):\n",
    "    \n",
    "    datio_path = DatioFileSystem().get().qualify(p_path)\n",
    "    fs = datio_path.fileSystem()\n",
    "    path = datio_path.path()\n",
    "    path_list = fs.listStatus(path)\n",
    "    paths = [path.getPath().toString() for path in path_list] #listado de todos los paths de la ruta pasada\n",
    "    \n",
    "    l_fechas = [element.split(campo+'=')[1] for element in paths if campo in element] #listado de todas las fechas\n",
    "    return max(l_fechas) # fecha mayor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d126ffae-1530-48a6-affb-ff5d1197663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formato fecha a formato string\n",
    "def date_to_str(fecha: date, mascara: str = \"%Y-%m-%d\") -> str:\n",
    "    try:\n",
    "        return format(fecha.strftime(mascara))\n",
    "    except ValueError as e:\n",
    "        raise Exception(\n",
    "            'Ha habido un error con la m√°scara ' + mascara + ' definida para la fecha ' + str(fecha) + ' \\n {}'.format(\n",
    "                str(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd79004f-4f25-4787-b128-bd5a699534a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formato string a formato fecha\n",
    "def str_to_date(fecha: str, mascara: str = \"%Y-%m-%d\") -> datetime:\n",
    "    try:\n",
    "        return datetime.strptime(fecha, mascara)\n",
    "    except ValueError as e:\n",
    "        raise Exception(\n",
    "            'Ha habido un error con la m√°scara ' + mascara + ' definida para la fecha ' + fecha + ' \\n {}'.format(\n",
    "                str(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e4f852b-8b3f-42c1-8fd9-84b1b239e2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_fecha (fecha_ini:str, ndays:int, op:str='add'):\n",
    "    if(op=='add'):\n",
    "        d = fecha_ini + timedelta(days=ndays)\n",
    "    else:\n",
    "        d = fecha_ini - timedelta(days=ndays)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbb15583-34f9-4f78-85e6-c1f92f694a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_fecha(2024-06-24,365) # datetime.date(2025, 6, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "686647c2-e048-41ab-be53-377ac01c911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si es un valor num√©rico lo pasamos a int\n",
    "import re\n",
    "\n",
    "def value_cast(p):\n",
    "    patron_numero = r'^(0|[1-9][0-9]*)$'\n",
    "    pv=re.findall(patron_numero,p)\n",
    "    if((pv==None) | (len(pv)==0)):\n",
    "        return p\n",
    "    else:\n",
    "        return int(p)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a2a4d-3425-45d7-9fc6-8204bfbe5728",
   "metadata": {},
   "source": [
    "# 1. Pto Partida\n",
    "- operaciones disponibles a carterizar\n",
    "- limites establecidos para la titulizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de57499f-1dad-4065-a130-68d4db036640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha de ejecuci√≥n del modelo titulizaci√≥n: 2024-10-22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/cartera_optima/closing_date=2024-10-22'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fecha_ejecucion = last_partition (root_path, 'closing_date')\n",
    "print('Fecha de ejecuci√≥n del modelo titulizaci√≥n:', fecha_ejecucion)\n",
    "root_pathc = root_path + 'closing_date=' + str(fecha_ejecucion)\n",
    "root_pathc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "648bcf73-f26a-4e08-ac8e-2525b70d9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actualizamos la fecha base\n",
    "fecha = fecha_ejecucion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9867ba-5a41-45e5-978b-2e90822b1412",
   "metadata": {},
   "source": [
    "## Limites\n",
    "los limites current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "311744eb-e2d6-47b8-b7d3-9d392f01c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "foto_limites= ['name_list_desc', 'limit_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3331f36-8b90-4e98-90d1-6ebf5128dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos los l√≠mites fijados y recalculamos el limit_value de aquellos necesarios\n",
    "path_limites_only = root_pathc + '/limites'\n",
    "limites_or = dataproc.read().parquet(path_limites_only)\n",
    "limites = get_limit_value(limites_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0c8009e-039a-4d67-92ce-9564eddfa728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limites_or.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8b52f3e-1c00-4185-899c-f621795982b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------+------------------+--------------+---------------------------------------+-------------+--------------+-----------+-------------------+--------------------+-----------+-----------+------------+-------------+--------+------------+\n",
      "|name_list_desc           |limit_date|limit_type        |concept1_desc |concept1_value                         |concept2_desc|concept2_value|limit_value|corporate_loan_flag|project_finance_flag|limit_scope|active_flag|visual_order|complex_limit|id_limit|closing_date|\n",
      "+-------------------------+----------+------------------+--------------+---------------------------------------+-------------+--------------+-----------+-------------------+--------------------+-----------+-----------+------------+-------------+--------+------------+\n",
      "|escenario model verano IV|2024-06-24|customer_subsector|subsector_desc|Paper, plastic, metal & glass packaging|null         |null          |0.5        |1                  |0                   |portfolio  |1          |0           |0            |150     |2024-10-15  |\n",
      "+-------------------------+----------+------------------+--------------+---------------------------------------+-------------+--------------+-----------+-------------------+--------------------+-----------+-----------+------------+-------------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limites.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b404cba-cfc7-45a2-ae7e-c9d491eefebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ya viene con el valor final 1-%launchpad\n",
    "# limites.where(F.col('limit_type')=='risk_retention').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3ef9997-dda5-46f8-a211-7da78001c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limites.where(F.col('limit_type')=='sts_group').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61fe510c-fb80-469a-918d-9f741b25d573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "escenario: escenario model verano IV\n",
      "fecha titulizacion 2024-06-24\n",
      "numero de limites: 216\n"
     ]
    }
   ],
   "source": [
    "escenario, date_titulizacion = [(x.name_list_desc,x.limit_date) for x in limites.select(*foto_limites).distinct().collect()][0]\n",
    "print('escenario:',escenario)\n",
    "print('fecha titulizacion',date_titulizacion)\n",
    "print('numero de limites:',limites.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166bbc1-9a3b-46c6-9f7e-b31144995183",
   "metadata": {},
   "source": [
    "### Tipo titulizacion\n",
    "marca las facilities que entran: solos las marcadas con Corporate/Project seg√∫n corresponda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa9fbb6d-9c5d-4e41-b787-49c0708b57bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titulizacion de corporate loan\n"
     ]
    }
   ],
   "source": [
    "tipo_titulizacion = limites.where(F.col('limit_type')=='portfolio_type').select('corporate_loan_flag','project_finance_flag')\n",
    "corporate_flag = tipo_titulizacion.select('corporate_loan_flag').collect()[0].corporate_loan_flag\n",
    "project_flag = tipo_titulizacion.select('project_finance_flag').collect()[0].project_finance_flag\n",
    "\n",
    "if(corporate_flag==1):\n",
    "    tipo_f='corporate_loan'\n",
    "    print('titulizacion de corporate loan')\n",
    "if(project_flag==1):\n",
    "    tipo_f='project_finance'\n",
    "    print('titulizacion de project finance')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f20d1442-7f56-46db-aa6c-b9c5f20e22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limite = 'risk_retention'\n",
    "# limites.where(F.col('limit_type')==limite).show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc233c6-5869-42da-a05f-7fe3fc0ee598",
   "metadata": {},
   "source": [
    "## Relaci√≥n limite - campo\n",
    "- cada l√≠mite el campo de la tabla de operaciones al que aplica\n",
    "\n",
    "(*) es una tabla que se sube como un cat√°logo directamente al Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3398422f-0f4e-412a-ae43-384d36653fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/limites/campos_datio/current/limites_camposDatio.csv'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37347b18-463f-4094-b3f3-8d231d5e9def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------+-----------+-----------+-----------+\n",
      "|limit_type    |campo_datio     |complex_flag|header_flag|imp_limit  |null_values|\n",
      "+--------------+----------------+------------+-----------+-----------+-----------+\n",
      "|portfolio_size|gf_ma_ead_amount|0           |1          |limit_scope|0          |\n",
      "|portfolio_date|clan_date       |0           |1          |limit_scope|0          |\n",
      "|portfolio_type|com_product     |0           |1          |limit_scope|0          |\n",
      "|risk_retention|Total_Amount_EUR|0           |0          |limit_scope|0          |\n",
      "|non_ig        |non_ig_flag     |0           |0          |limit_scope|0          |\n",
      "+--------------+----------------+------------+-----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limite_campo = spark.read.option('header','True').option('delimiter',';').csv(path_campos)\n",
    "# limite_campo.show(5,False)\n",
    "\n",
    "# depende de la tipolog√≠a de la titulizaci√≥n cogemos una columna y otra para la referencia al campo Datio\n",
    "if(corporate_flag==1):\n",
    "    df_campos = limite_campo.drop('project_finance_column'\n",
    "                                 ).withColumnRenamed('corporate_loan_column','campo_datio')\n",
    "else:\n",
    "    df_campos = limite_campo.drop('corporate_loan_column'\n",
    "                                 ).withColumnRenamed('project_finance_column','campo_datio')\n",
    "df_campos.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31d9081d-d8e7-4cfd-b38c-bfa2eaa1715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limite_campo.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56631588-0716-471d-8389-c6b83cee6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex_flag si el limite afecta a 2 campos de Datio\n",
    "# df_campos.where(F.col('complex_flag')==1).show(2,False)\n",
    "# +-------------------------+-------------------------------------------------------------+------------+\n",
    "# |limit_type               |campo_datio                                                  |complex_flag|\n",
    "# +-------------------------+-------------------------------------------------------------+------------+\n",
    "# |project_sector_subsector |project_sector_desc/project_subsector_desc                   |1           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b080f40-8607-4da7-9a97-73b186675b3a",
   "metadata": {},
   "source": [
    "#### Listado de campos Datio\n",
    "dejamos en una lista todos los campos que estan implicados en los l√≠mites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d3789e9-c31e-4f8f-8cc1-5fc52d4299f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dejamos el listado de todos los campos de Datio que se tienen en cuenta (Ej.project_sector_desc,project_subsector_desc)\n",
    "cols_datio = [x.campo_datio.split('/')[0] for x in df_campos.select('campo_datio').distinct().collect()]\n",
    "# cols_datio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485b9ea-eb0f-46cd-9093-45cfe25e384f",
   "metadata": {},
   "source": [
    "### Fusionamos informacion Limites\n",
    "A√±adimos una columnas que por l√≠mite sepamos el campo Datio al que hace referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11954694-b905-4958-a036-ea3e25e33091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------+----------+--------------+----------------------------+-------------+--------------+-----------+-------------------+--------------------+-----------+-----------+------------+-------------+--------+------------+------------------------------+------------+-----------+----------+-----------+\n",
      "|limit_type        |name_list_desc           |limit_date|concept1_desc |concept1_value              |concept2_desc|concept2_value|limit_value|corporate_loan_flag|project_finance_flag|limit_scope|active_flag|visual_order|complex_limit|id_limit|closing_date|campo_datio                   |complex_flag|header_flag|imp_limit |null_values|\n",
      "+------------------+-------------------------+----------+--------------+----------------------------+-------------+--------------+-----------+-------------------+--------------------+-----------+-----------+------------+-------------+--------+------------+------------------------------+------------+-----------+----------+-----------+\n",
      "|customer_country  |escenario model verano IV|2024-06-24|country_desc  |United Kingdom              |null         |null          |0.2        |1                  |0                   |portfolio  |1          |0           |0            |74      |2024-10-15  |customer_country              |0           |0          |portfolio |0          |\n",
      "|rating_rga_esl    |escenario model verano IV|2024-06-24|rating_large  |BBB1                        |null         |null          |0.0075     |1                  |1                   |individual |1          |0           |0            |19      |2024-10-15  |gf_ma_expanded_master_scale_id|0           |0          |portfolio |0          |\n",
      "|customer_subsector|escenario model verano IV|2024-06-24|subsector_desc|Road and rail transportation|null         |null          |0.5        |1                  |0                   |portfolio  |1          |0           |0            |155     |2024-10-15  |g_asset_allocation_subsec_desc|0           |0          |portfolio |0          |\n",
      "|rating_rga_esl    |escenario model verano IV|2024-06-24|rating_large  |BB+2                        |null         |null          |0.0025     |1                  |1                   |individual |1          |0           |0            |24      |2024-10-15  |gf_ma_expanded_master_scale_id|0           |0          |portfolio |0          |\n",
      "|rating_sp         |escenario model verano IV|2024-06-24|rating_group  |CCC-                        |null         |null          |0.0        |1                  |1                   |individual |1          |0           |0            |39      |2024-10-15  |group_rating_sp               |0           |0          |individual|1          |\n",
      "+------------------+-------------------------+----------+--------------+----------------------------+-------------+--------------+-----------+-------------------+--------------------+-----------+-----------+------------+-------------+--------+------------+------------------------------+------------+-----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limites_total = limites.join(df_campos,['limit_type'],'left').dropDuplicates(\n",
    ").withColumn('imp_limit', F.when(F.col('imp_limit')=='limit_scope', F.col('limit_scope')).otherwise(F.col('imp_limit')))\n",
    "\n",
    "limites_total.show(5,False)\n",
    "# si es un limite compuesto en la columna campo Datio campo1/campo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0ae711e-c1fa-4c84-a35b-dfe4e3bb3b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limite = 'risk_retention'\n",
    "# limites_total.where(F.col('limit_type')==limite).show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5231dfa-f2a7-4072-b29b-6dba900eafa4",
   "metadata": {},
   "source": [
    "## Operaciones\n",
    "la ultima foto disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c8f4ba3-8f7a-4682-a604-cdc1024471bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_facilities = root_pathc + '/facilities'\n",
    "facilities_0 = dataproc.read().parquet(path_facilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c82eafb9-fe7a-4534-929f-8dde696efa65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/sandboxes/dslb/data/Joystick/TITULIZACIONES/cartera_optima/closing_date=2024-10-22/facilities'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f977838c-ea1e-4d65-98f9-361b22f8e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#facilities.groupBy('com_product').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0573e961-4f21-4aa6-8d4b-838e7ad6af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_tipo_facility[tipo_f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c1c5e-01cf-4b84-b9f2-f17bc3a08239",
   "metadata": {},
   "source": [
    "### Facilities tipo\n",
    "solo cogemos las facilities del tipo de titulizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8629604-9539-4a9e-a6d7-da99ce2344c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities = facilities_0.where(F.col('com_product')==d_tipo_facility[tipo_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "006b3fe7-c617-4e13-9f32-5907c6580fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos a nivel facility\n",
      "numero de operaciones de base: 1023\n",
      "fecha foto facilities: 2024-10-21\n"
     ]
    }
   ],
   "source": [
    "if(facilities.count()==facilities.select(*key_facility).distinct().count()):\n",
    "    print('Datos a nivel facility')\n",
    "fecha_facilities = date_to_str([x.data_date for x in facilities.select('data_date').distinct().collect()][0])\n",
    "    \n",
    "print('numero de operaciones de base:', facilities.select(*key_facility).distinct().count())\n",
    "print('fecha foto facilities:',fecha_facilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911c5a3-5d01-4c8a-8a20-d4a51e084418",
   "metadata": {},
   "source": [
    "### A√±adimos Columnas Traza\n",
    "- excluded: operaci√≥n excluida, se marca con flag 1/0 para indicar si se excluye por los l√≠mites\n",
    "- exclusion_limit: limite que marca su exclusi√≥n de la cartera √≥ptima\n",
    "\n",
    "Se inicializan a excluded=0 y exclusion_limit=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e248d061-94f5-4dc7-b763-1b2551f976f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_traza = ['excluded','exclusion_limit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a653725a-6a00-4d6b-b75a-3a649fef928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_t = facilities.withColumn('excluded', F.lit(0)\n",
    "                        ).withColumn('exclusion_limit', F.lit(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16ba837b-01a7-473c-be31-199ff5acab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(facilities.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffd2be45-3942-48ca-8828-407cee2fdb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------+------------------+---------+------------------------+----------------------+--------------------------+--------------+------------+-----------+-----------------+---------------+----------------------------+-----------+------------------------+------------------+------------------------+---------------------------+------------------------------+-------------------------------+---------------+-------------------------+------------------+---------------+--------------------+------------------------------+------------------------------+----------------+--------------------+-----------------+---------------------+---------------------+-------------------------+---------------+-----------------------------+--------------------------+------------------------+----------------------------+---------------------+--------------------+----------------------+---------------------------+--------------------------+--------------------------+----------------+----------------+----------------+--------+--------+-----------+--------+----------+------------------------+-----------------------+-----------------------+-------------+----------+----------+---------------+-----------------+----------------------------------------+---------------+---------------------+---------------+------------+-------------------+---------------------------------+---------------------+-------------------------+------------------+----------+-------------+----------+---------+----------------------+--------+-----------+---------------------+------------+----------------+--------------+---------------+--------+----------+--------+---------------+\n",
      "|project_sector_desc    |delta_file_id|delta_file_band_id|branch_id|project_country_desc    |financial_product_desc|deal_purpose_type         |seniority_name|insured_type|currency_id|deal_signing_date|expiration_date|financial_product_class_desc|customer_id|customer_country        |g_holding_group_id|group_country_desc      |m5_expanded_master_scale_id|gf_ma_expanded_master_scale_id|ma_expanded_master_scale_number|pd_ma_mitig_per|gf_m5_economic_ead_amount|gf_ma_ead_amount  |pd_m5_mitig_per|adj_lgd_ma_mitig_per|g_asset_allocation_sector_desc|g_asset_allocation_subsec_desc|final_stage_type|com_product         |bbva_drawn_amount|bbva_available_amount|bbva_drawn_eur_amount|bbva_available_eur_amount|group_rating_sp|watch_list_clasification_type|gf_capital_adjustment_desc|gf_pf_project_const_type|gf_sbprfl_mrch_risk_ind_type|gf_pf_current_ratg_id|gf_pf_score_ind_desc|gf_pf_final_lgd_amount|gf_current_rating_tool_date|g_smscl_internal_ratg_type|g_lmscl_internal_ratg_type|exchange_rate   |Total_Amount_CCY|Total_Amount_EUR|EC_per  |RC_per  |Risk_Weight|EL_per  |Reg_EL_per|Internal_Rating_EC_Model|Importe_Garantizado_CCY|Importe_Garantizado_EUR|basemoto_date|ifrs9_date|project_id|g_customer_id  |banking_entity_id|banking_entity_desc                     |syndicated_type|sts_payment_condition|gf_pf_ratg_date|gf_rw_sm_per|sts_sm_rw_condition|gf_facility_securitization_amount|bei_guaranteed_amount|non_bei_guaranteed_amount|plazo_medio       |ind_rating|ind_inv_grade|esg_linked|clan_date|project_subsector_desc|ico_flag|non_ig_flag|building_project_flag|workout_flag|sts_payment_flag|sts_sm_rw_flag|esg_linked_flag|bei_flag|data_date |excluded|exclusion_limit|\n",
      "+-----------------------+-------------+------------------+---------+------------------------+----------------------+--------------------------+--------------+------------+-----------+-----------------+---------------+----------------------------+-----------+------------------------+------------------+------------------------+---------------------------+------------------------------+-------------------------------+---------------+-------------------------+------------------+---------------+--------------------+------------------------------+------------------------------+----------------+--------------------+-----------------+---------------------+---------------------+-------------------------+---------------+-----------------------------+--------------------------+------------------------+----------------------------+---------------------+--------------------+----------------------+---------------------------+--------------------------+--------------------------+----------------+----------------+----------------+--------+--------+-----------+--------+----------+------------------------+-----------------------+-----------------------+-------------+----------+----------+---------------+-----------------+----------------------------------------+---------------+---------------------+---------------+------------+-------------------+---------------------------------+---------------------+-------------------------+------------------+----------+-------------+----------+---------+----------------------+--------+-----------+---------------------+------------+----------------+--------------+---------------+--------+----------+--------+---------------+\n",
      "|CORPORATE              |813008       |0                 |7803     |France                  |RCF                   |General Corporate Purposes|Senior        |Unsecured   |EUR        |2022-04-14       |2025-04-14     |Back-up Line Committed      |032736930  |France                  |G20081107180739   |France                  |A                          |A                             |06                             |0.000880       |57000000.00000000        |112500000.00000000|0.000800       |0.450000000         |Financial Services            |Insurance companies           |1               |Corporate Facilities|0.000000         |150000000.000000     |0.000000             |150000000.000000         |A+             |0                            |No Informado              |No Informado            |No Informado                |No Informado         |No Informado        |null                  |2024-06-12                 |A                         |A                         |1.00000000000000|150000000.000000|150000000.000000|0.010429|0.014445|0.1805625  |0.000360|0.000396  |NA                      |0                      |0                      |2024-09-30   |20240930  |195061    |ES0182032736930|0182             |BANCO BILBAO VIZCAYA ARGENTARIA S.A.    |Bilateral      |true                 |No Informado   |0.200000    |true               |0.000000                         |null                 |null                     |0.0               |06        |1            |1         |20241021 |No Informado          |0       |0          |0                    |0           |1               |1             |1              |0       |2024-10-21|0       |               |\n",
      "|Corporate Asset Finance|824210       |0                 |7590     |United States of America|Term Loan             |Purchase of Assets        |Senior        |Secured     |USD        |2022-12-12       |2025-08-08     |Bullet                      |059246078  |United States of America|G20231113120239   |United States of America|BBB1                       |BBB-1                         |15                             |0.002400       |85394957.27000000        |85394957.27000000 |0.001800       |0.450000000         |Institutions                  |Other Institutions            |1               |Corporate Facilities|95286886.780000  |0.000000             |87797739.590000      |0.000000                 |No Informado   |0                            |No Informado              |No Informado            |No Informado                |No Informado         |No Informado        |null                  |2023-11-16                 |BBB                       |BBB1                      |1.08530000000336|95286886.780000 |87797739.590000 |0.032800|0.040000|0.5        |0.000000|0.000000  |NA                      |0                      |0                      |2024-09-30   |20240930  |203800    |ES0182059246078|0182             |BANCO BILBAO VIZCAYA ARGENTARIA S.A.    |Syndicated     |true                 |No Informado   |1.000000    |true               |0.000000                         |null                 |null                     |2.6575342465753424|12        |0            |0         |20241021 |No Informado          |0       |0          |0                    |0           |1               |1             |0              |0       |2024-10-21|0       |               |\n",
      "+-----------------------+-------------+------------------+---------+------------------------+----------------------+--------------------------+--------------+------------+-----------+-----------------+---------------+----------------------------+-----------+------------------------+------------------+------------------------+---------------------------+------------------------------+-------------------------------+---------------+-------------------------+------------------+---------------+--------------------+------------------------------+------------------------------+----------------+--------------------+-----------------+---------------------+---------------------+-------------------------+---------------+-----------------------------+--------------------------+------------------------+----------------------------+---------------------+--------------------+----------------------+---------------------------+--------------------------+--------------------------+----------------+----------------+----------------+--------+--------+-----------+--------+----------+------------------------+-----------------------+-----------------------+-------------+----------+----------+---------------+-----------------+----------------------------------------+---------------+---------------------+---------------+------------+-------------------+---------------------------------+---------------------+-------------------------+------------------+----------+-------------+----------+---------+----------------------+--------+-----------+---------------------+------------+----------------+--------------+---------------+--------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "facilities_t.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68224820-9474-4861-b65b-7edb5ec3df2c",
   "metadata": {},
   "source": [
    "## Portfolio Size\n",
    "el importe total que se quiere titulizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69ac53b8-19a9-4fe1-b59a-fffeb8411f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importe a portfolio size total: 2000000000.0\n"
     ]
    }
   ],
   "source": [
    "portfolio_size = [x.limit_value for x in limites_total.where(F.col('limit_type')=='portfolio_size').select('limit_value').collect()][0]\n",
    "print('Importe a portfolio size total:', portfolio_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521de6b1-a128-4bd5-8195-dd8e8b8340c0",
   "metadata": {},
   "source": [
    "# 2. Importe Susceptible (importe base)\n",
    "Por cada facility: Calculamos aplicando f√≥rmula de la hoja t√°ctica, que parte de la facility se podr√≠a titulizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90db45-93ee-4f8a-b364-a53d90165ae1",
   "metadata": {},
   "source": [
    "## Aplicamos f√≥rmula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a8ce9-c934-4a04-ad2f-1c7296de11f5",
   "metadata": {},
   "source": [
    "**importe_susceptible = (min(ead_regulatorio, (saldo vivo+importe disponible*CCF))) - (importe_titulizado/risk_retention)**\n",
    "- ead_regulatorio -> gf_ma_ead_amount\n",
    "- saldo vivo -> bbva_drawn_eur_amount (saldo dispuesto - amortizado). Es directamente el dato del dispuesto en euros\n",
    "- importe disponible -> bbva_available_eur_amount (importe en oficina en euros)\n",
    "- CCF : dentro del launchpad de limites (limit_type=='ccf'). Es el factor de conversi√≥n con el que se calcula el EAD\n",
    "- importe titulizado -> gf_facility_securitization_amount\n",
    "- risk retention : dentro del launchpad de limite (limit_type=='risk_retention'). Ya est√° calculado su valor con (1 - %del excel), que ser√≠a el % a aplicar\n",
    "\n",
    "= +MIN(AU81;U81+75%*V81)-SI.ERROR(BUSCARV( ùëç81;‚Ä≤ùëâùëíùëüùëéùëõùëúùêºùëÖùëÖ‚Ä≤! B:$C;2;0)/80%;0) = min(EADReg, Saldo Vivo BBVA + Importe Disponible BBVA_euros * 0.75) - (Reference Obligation Notional Amount/0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797c9bf-8678-4332-9852-31c8ba94ccf3",
   "metadata": {},
   "source": [
    "### L√≠mites necesarios \n",
    "Incluimos columnas limites necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3727c14a-c9dd-4824-8fe0-14f99df3fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# como hay limites que se aplican directamente en alguna f√≥rmula los identificamos para saber que ya se han aplicado\n",
    "lim_usados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1f92783-0a32-402f-abc8-6f9cfbf7d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_imp = ['risk_retention', 'ccf']\n",
    "lim_usados = lim_usados + lim_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f8a8a37-c622-4637-953d-17025da13084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('risk_retention', 0.800000011920929), ('ccf', 0.75)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [(x.limit_type, x.limit_value) for x in limites_total.select('limit_type','limit_value').where(F.col('limit_type').isin(*lim_imp)).collect()]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce5d4220-9e92-458c-a856-9f36b1499240",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_t1 = facilities_t.withColumn('limit_escenario',F.lit(escenario)\n",
    "                           ).withColumn('limit_fecha',F.lit(date_titulizacion))\n",
    "for e in l:\n",
    "    facilities_t1 = facilities_t1.withColumn('limit_'+e[0],F.lit(e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28384c33-c697-41a6-8cd8-6be0a89fa4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|limit_ccf|limit_risk_retention|\n",
      "+---------+--------------------+\n",
      "|0.75     |0.800000011920929   |\n",
      "|0.75     |0.800000011920929   |\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "facilities_t1.select('limit_ccf','limit_risk_retention').show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82051752-bc54-47c1-b179-d6a61c4f49c2",
   "metadata": {},
   "source": [
    "### C√°lculo del importe\n",
    "Se incluye una nueva columna con el importe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bbfd46-0868-4352-beae-5c91393858af",
   "metadata": {},
   "source": [
    "importe_susceptible = (min(ead_regulatorio, (saldo vivo+importe disponible*CCF))) - (importe_titulizado/risk_retention).\n",
    "\n",
    "Lo dividimos en partes:\n",
    "- importe1 = saldo vivo+importe disponible*CCF\n",
    "- importe2 = importe_titulizado/risk_retention\n",
    "- importe_susceptible = min(ead_regulatorio,importe1) - importe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b43233ae-518c-4155-952b-97926b3dbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_f0 = facilities_t1.withColumn('importe1', F.col('bbva_drawn_eur_amount')+(F.col('bbva_available_eur_amount')*F.col('limit_ccf'))\n",
    "                                           ).withColumn('importe2', F.col('gf_facility_securitization_amount')/F.col('limit_risk_retention')\n",
    "                                           ).withColumn('importe_susceptible', F.least(F.col('gf_ma_ead_amount'),F.col('importe1')) - F.col('importe2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6d4c413-275b-4cda-a7c8-bf3a02af8281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+---------+---------------------+-------------------------+---------+--------+---------------------------------+--------------------+--------+----------------+-------------------+\n",
      "|delta_file_id|delta_file_band_id|branch_id|bbva_drawn_eur_amount|bbva_available_eur_amount|limit_ccf|importe1|gf_facility_securitization_amount|limit_risk_retention|importe2|gf_ma_ead_amount|importe_susceptible|\n",
      "+-------------+------------------+---------+---------------------+-------------------------+---------+--------+---------------------------------+--------------------+--------+----------------+-------------------+\n",
      "+-------------+------------------+---------+---------------------+-------------------------+---------+--------+---------------------------------+--------------------+--------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_test = '366005' # '815974' #'415725'\n",
    "\n",
    "cols_f0 = ['bbva_drawn_eur_amount','bbva_available_eur_amount','limit_ccf','importe1',\n",
    "          'gf_facility_securitization_amount','limit_risk_retention','importe2',\n",
    "          'gf_ma_ead_amount','importe_susceptible']\n",
    "\n",
    "facilities_f0.select(*key_facility,*cols_f0).where(F.col('delta_file_id')==id_test).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2abee98-bcdb-48b8-a993-0fc96f7d73d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facilities totales: 1023\n",
      "Facilities con importe a titulizar: 917\n"
     ]
    }
   ],
   "source": [
    "print('Facilities totales:', facilities_f0.count())\n",
    "print('Facilities con importe a titulizar:', facilities_f0.where(F.col('importe_susceptible')>0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ea5a33e-5950-47ce-b15b-4208c097a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facilities_f1.where(F.col('delta_file_id')=='415725').show(5,False)\n",
    "# tramo 1: 24.750.854,13\n",
    "# tramo 3: 366.875,99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad81fb-7333-478d-8168-939a205b32af",
   "metadata": {},
   "source": [
    "**(*)otra formula: min((saldo_vivo - Titulizado/risk_retention), (ead/risk_retention))**\n",
    "- importe1a = saldo_vivo - Titulizado/risk_retention . (*) no s√© si ccf o risk_retention, en la hoja aplica directamente un 0.8\n",
    "- importe2a = ead/risk_retention\n",
    "- importe_susceptible = min(importe1a,importe2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4cc0ad4-44ee-4b7b-87d6-eff8283e59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUEVOS CAMPOS: para esta f√≥rmula si fueran necesarios\n",
    "# facilities_f1 = facilities_f0.withColumn('importe1_v1', (F.col('bbva_drawn_eur_amount') - (F.col('gf_facility_securitization_amount')/F.col('limit_risk_retention')))\n",
    "#                                     ).withColumn('importe2_v1', F.col('gf_ma_ead_amount')/F.col('limit_risk_retention')\n",
    "#                                     ).withColumn('importe_susceptible_v1',F.least('importe1_v1','importe2_v1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e72c4578-7ea7-4e9c-87c7-d65522c76f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_fa = ['bbva_drawn_eur_amount','gf_facility_securitization_amount','limit_risk_retention','importe1_v1',\n",
    "#           'gf_ma_ead_amount','importe2_v1','importe_susceptible_v1']\n",
    "\n",
    "# facilities_f1.select(*key_facility,*cols_fa).where(F.col('delta_file_id')==id_test).show(5,False)                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b97ea32f-f855-4bf6-9ef0-a2ba81af7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# cls = list(set(key_facility + cols_f0 + cols_fa))\n",
    "# facilities_f1.where(F.col('importe_susceptible_v1')!=F.col('importe_susceptible')).count() # 886\n",
    "# facilities_f1.where(F.col('importe_susceptible_v1')!=F.col('importe_susceptible')).select(*cls).orderBy(*key_facility).show(5,False)\n",
    "# facilities_f1.where((F.col('importe_susceptible_v1')>0) & (F.col('importe_susceptible')==0)).count() # 0\n",
    "# facilities_f1.where((F.col('importe_susceptible')>0) & (F.col('importe_susceptible_v1')==0)).count() # 379"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c7283-b6db-45b8-a282-d101ef290587",
   "metadata": {},
   "source": [
    "# 3. L√≠mites Individuales\n",
    "Aplicamos limites √°mbito individual: min(%limit_value) \n",
    "\n",
    "- a√±adimos columna con el % que se aplica por cada l√≠mite individual\n",
    "- a√±adimos columna con el l√≠mite individual m√°s restrictivo (el de menor % en el l√≠mite)\n",
    "- a√±adimos columna con el % del valor del l√≠mite individual m√°s restrictivo (menor %)\n",
    "- a√±adimos columna con el importe titulizable de cada facility : importe susceptible a titulizar * min(%individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d34be57-fa4d-403e-96ec-57753c78ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+------------------------------+-----------+\n",
      "|limit_type         |concept1_desc    |campo_datio                   |limit_scope|\n",
      "+-------------------+-----------------+------------------------------+-----------+\n",
      "|bei                |bei_flag         |delta_file_id                 |individual |\n",
      "|excluded_facilities|facility_id      |delta_file_id                 |individual |\n",
      "|maturity_min       |num_days         |expiration_date               |individual |\n",
      "|rating_rga_esl     |rating_large     |gf_ma_expanded_master_scale_id|individual |\n",
      "|rating_sp          |rating_group     |group_rating_sp               |individual |\n",
      "|risk_retention     |risk_bbva_percent|Total_Amount_EUR              |individual |\n",
      "|sts_payment        |sts_flag         |sts_payment_flag              |individual |\n",
      "|sts_rw_modelo      |sts_flag         |sts_sm_rw_flag                |individual |\n",
      "+-------------------+-----------------+------------------------------+-----------+\n",
      "\n",
      "existen filtros complejos: definici√≥n de 2 niveles de conceptos\n",
      "+-------------+-------------+---------------------+----------------+-----------+\n",
      "|limit_type   |concept1_desc|concept2_desc        |campo_datio     |limit_scope|\n",
      "+-------------+-------------+---------------------+----------------+-----------+\n",
      "|sts_payment  |sts_flag     |sts_payment_condition|sts_payment_flag|individual |\n",
      "|sts_rw_modelo|sts_flag     |sts_sm_rw_condition  |sts_sm_rw_flag  |individual |\n",
      "+-------------+-------------+---------------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limites_ind = limites_total.where(F.col('limit_scope')=='individual')\n",
    "limites_ind.select('limit_type','concept1_desc','campo_datio','limit_scope').distinct().orderBy('limit_type').show(limites_ind.count(),False)\n",
    "\n",
    "if(limites_ind.where(F.col('complex_limit')==1).count()>0):\n",
    "    print('existen filtros complejos: definici√≥n de 2 niveles de conceptos')\n",
    "    limites_ind.where(F.col('complex_limit')==1\n",
    "                    ).select('limit_type','concept1_desc','concept2_desc','campo_datio','limit_scope').distinct().orderBy('limit_type').show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525dd5d3-1315-4f87-b042-b425c2583e25",
   "metadata": {},
   "source": [
    "### Diccionario\n",
    "correspondencia entre: limite - campo Datio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0daaf2c3-82cc-4770-b481-ae4fad363c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit_ind.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "567fa165-86a8-4cdc-bcf8-2a22cb46df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limites simples\n",
    "lista =limites_ind.select('limit_type','campo_datio').collect()\n",
    "dict_lim_ind = {row['limit_type']: row['campo_datio'] for row in lista}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8148453c-ae2a-451c-8ed3-af2f01c92429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rating_rga_esl': 'gf_ma_expanded_master_scale_id',\n",
       " 'rating_sp': 'group_rating_sp',\n",
       " 'excluded_facilities': 'delta_file_id',\n",
       " 'risk_retention': 'Total_Amount_EUR',\n",
       " 'maturity_min': 'expiration_date',\n",
       " 'sts_rw_modelo': 'sts_sm_rw_flag',\n",
       " 'bei': 'delta_file_id',\n",
       " 'sts_payment': 'sts_payment_flag'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_lim_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6199ca6-267b-45de-9f6e-4f619bad5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_lim_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c227c4c8-19c7-4ffd-91e4-73ca0ab36d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dict_lim_ind_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86295463-03fb-4479-9c61-89ea1e750556",
   "metadata": {},
   "source": [
    "### Columnas valor nulo\n",
    "para campos a nulos que % del limite se aplica. Por defecto limite aplica a 0% si la columna tiene valor nulo\n",
    "-  viene marcado en el fichero limite-campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0cef212-d5e1-41c1-9b1d-79e28845b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "l =limites_ind.select('limit_type','null_values').collect()\n",
    "dict_lim_ind_nul = {row['limit_type']: row['null_values'] for row in l}\n",
    "# dict_lim_ind_nul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec6577-5646-4bb6-8b68-d99cea1bc140",
   "metadata": {},
   "source": [
    "### Limites por Facility\n",
    "- generamos columna con lista de limites aplicados\n",
    "- generamos las columnas con el valor de los limites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4320eef9-31f6-4c02-b8e5-69910ce564f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** limite analizado: rating_rga_esl\n",
      "campo de la facility al que aplica: gf_ma_expanded_master_scale_id\n",
      "*** limite analizado: rating_sp\n",
      "campo de la facility al que aplica: group_rating_sp\n",
      "*** limite analizado: excluded_facilities\n",
      "campo de la facility al que aplica: delta_file_id\n",
      "*** limite analizado: risk_retention\n",
      "campo de la facility al que aplica: Total_Amount_EUR\n",
      "limit simple con limit value total: limit_risk_retention\n",
      "*** limite analizado: maturity_min\n",
      "campo de la facility al que aplica: expiration_date\n",
      "*** limite analizado: sts_rw_modelo\n",
      "campo de la facility al que aplica: sts_sm_rw_flag\n",
      "*** limite analizado: bei\n",
      "campo de la facility al que aplica: delta_file_id\n",
      "*** limite analizado: sts_payment\n",
      "campo de la facility al que aplica: sts_payment_flag\n"
     ]
    }
   ],
   "source": [
    "facilities_add = facilities_f0.withColumn('limits_applied',F.lit('')) # tomamos de base las facilities y vamos a√±adiendo columnas y comprobaciones\n",
    "varios_campos = 0 # partimos de la base que s√≥lo afecta a un campo de Datio\n",
    "limits_indv = []\n",
    "cols_facility = facilities_add.columns\n",
    "\n",
    "for k,v in dict_lim_ind.items():\n",
    "    print('*** limite analizado:',k)\n",
    "    print('campo de la facility al que aplica:',v)\n",
    "    \n",
    "    if('/' in v): # limite con varios campos viene con este separador en el cat√°logo de campos\n",
    "        varios_campos = 1\n",
    "        \n",
    "    # CASO BASE: casi todos lo l√≠mites aplican a un solo campo\n",
    "    if (varios_campos == 0):\n",
    "        df_limite1 = limites_ind.where(F.col('limit_type')==k\n",
    "                                            ).withColumn('concepto_valor',F.when(F.col('complex_limit')==1,F.col('concept2_value')).otherwise(F.col('concept1_value'))\n",
    "                                            ).select('concepto_valor','limit_value'\n",
    "                                            ).withColumnRenamed('concepto_valor' ,v\n",
    "                                            ).withColumnRenamed('limit_value' ,'limit_'+k\n",
    "                                            ).withColumn('limit_'+k,F.col('limit_'+k).cast(\"float\")\n",
    "                                            ).withColumn('limit_apply', F.lit('limit_'+k))\n",
    "        \n",
    "        \n",
    "        # limite generico para todas las facilities\n",
    "        if((df_limite1.count()==1)&(df_limite1.select(v).collect()[0][v]=='total')):\n",
    "            facilities_add = facilities_add.withColumn('limit_'+k,F.lit(df_limite1.select('limit_'+k).collect()[0]['limit_'+k]).cast(\"float\")\n",
    "                                                      ).withColumn('limit_apply', F.lit('limit_'+k))\n",
    "            print('limit simple con limit value total:','limit_'+k)\n",
    "        \n",
    "        # limite seg√∫n categoria - valor\n",
    "        else:\n",
    "            facilities_add = facilities_add.join(df_limite1, [v],'left').fillna(1.0).fillna({'limit_apply':''}) # si alg√∫n valor no vienen marcado en el l√≠mite se aplica un valor del 100% (1,0)\n",
    "            # valor del limite para columnas nulas\n",
    "            n0 = facilities_add.where(F.col(v).isNull()).count()\n",
    "            if(n0>0): # si hay columnas nulas para ese campo\n",
    "                print('facilities con columna:',v,'nula:',facilities_add.where(F.col(v).isNull()).count())\n",
    "                print('% de limite',k,'para estas facilities:',dict_lim_ind_nul[k])\n",
    "                facilities_add = facilities_add.withColumn('limit_'+k,F.when(F.col(v).isNull(),dict_lim_ind_nul[k]).otherwise(F.col('limit_'+k))) # pongo el limite establecido para las columnas nulas \n",
    "    \n",
    "    #CASO COMPLEJO: aplica a varios campos \n",
    "    else:\n",
    "        v1,v2 = v.split('/')\n",
    "        \n",
    "        df_limite2 = limites_ind.where(F.col('limit_type')==k\n",
    "                                            ).select('concept1_value','concept2_value','limit_value'\n",
    "                                            ).withColumnRenamed('concept1_value' ,v1\n",
    "                                            ).withColumnRenamed('concept2_value' ,v2\n",
    "                                            ).withColumnRenamed('limit_value' ,'limit_'+k\n",
    "                                            ).withColumn('limit_'+k,F.col('limit_'+k).cast(\"float\")\n",
    "                                            ).withColumn('limit_apply', F.lit('limit_'+k))\n",
    "        \n",
    "        # limite generico para todas las facilities\n",
    "        if((df_limite2.count()==1)&(df_limite2.select(v1).collect()[0][v1]=='total')):\n",
    "\n",
    "            if((df_limite2.count()==1)&(df_limite2.select(v2).collect()[0][v2]=='total')):\n",
    "                facilities_add = facilities_add.withColumn('limit_'+k,F.lit(df_limite2.select('limit_'+k).collect()[0]['limit_'+k]).cast(\"float\")\n",
    "                                                          ).withColumn('limit_apply', F.lit('limit_'+k))\n",
    "            else:\n",
    "                facilities_add = facilities_add.join(df_limite2, [v2],'left').fillna(1.0).fillna({'limit_apply':''}) # si alg√∫n valor no vienen marcado en el l√≠mite se aplica un valor del 100% (1,0)\n",
    "\n",
    "        # limite seg√∫n categoria - valor\n",
    "        else:\n",
    "            if((df_limite2.count()==1)&(df_limite2.select(v2).collect()[0][v2]=='total')):\n",
    "                facilities_add = facilities_add.join(df_limite2, [v1],'left').fillna(1.0).fillna({'limit_apply':''}) # si alg√∫n valor no vienen marcado en el l√≠mite se aplica un valor del 100% (1,0)\n",
    "            else:\n",
    "                facilities_add = facilities_add.join(df_limite2, [v1,v2],'left').fillna(1.0).fillna({'limit_apply':''}) # si alg√∫n valor no vienen marcado en el l√≠mite se aplica un valor del 100% (1,0)   \n",
    "                # valor del limite para columnas nulas\n",
    "                n1 = facilities_add.where(F.col(v2).isNull()).count()\n",
    "                if(n1>0): #si hay valores nulos para la columna v2\n",
    "                    print('facilities con columna:',v2,'nula:',facilities_add.where(F.col(v2).isNull()).count())\n",
    "                    print('% de limite',k,'para estas facilities:',dict_lim_ind_nul[k])\n",
    "                    facilities_add = facilities_add.withColumn('limit_'+k,F.when(F.col(v2).isNull(),dict_lim_ind_nul[k]).otherwise(F.col('limit_'+k))) # pongo el limite establecido para las columnas nulas \n",
    "    \n",
    "    # a√±adimos al campo de limites aplicados el procesado que vendr√° con valor cuando los limites han coincidido\n",
    "    facilities_add = facilities_add.withColumn('limits_applied',F.when(F.col('limit_apply')!='',F.concat(F.col('limits_applied'),F.lit(','),F.col('limit_apply'))\n",
    "                                                                      ).otherwise(F.col('limits_applied'))).drop('limit_apply'\n",
    "                                   ).withColumn('exclusion_limit', F.when(F.col('limit_'+k)==0,F.concat(F.lit(k),F.lit(', '),F.col('exclusion_limit'))\n",
    "                                                                    ).otherwise(F.col('exclusion_limit')))\n",
    "    limits_indv.append('limit_'+k)\n",
    "    varios_campos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "432f76df-129f-431b-9cc4-4f354ecf2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipos de agrupaciones de limites\n",
    "# facilities_add.groupBy('limits_applied').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "573c6eee-c7e9-48ce-8a7c-f9f53e552ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+---------+------------------------------------------+--------------------+---------------+-------------------------+--------------------+------------------+-------------------+---------+-----------------+------------------------------+---------------+-------------+----------------+---------------+--------------+-------------+----------------+\n",
      "|delta_file_id|delta_file_band_id|branch_id|limits_applied                            |limit_rating_rga_esl|limit_rating_sp|limit_excluded_facilities|limit_risk_retention|limit_maturity_min|limit_sts_rw_modelo|limit_bei|limit_sts_payment|gf_ma_expanded_master_scale_id|group_rating_sp|delta_file_id|Total_Amount_EUR|expiration_date|sts_sm_rw_flag|delta_file_id|sts_payment_flag|\n",
      "+-------------+------------------+---------+------------------------------------------+--------------------+---------------+-------------------------+--------------------+------------------+-------------------+---------+-----------------+------------------------------+---------------+-------------+----------------+---------------+--------------+-------------+----------------+\n",
      "|806487       |2                 |7701     |,limit_rating_rga_esl,limit_risk_retention|0.0025              |1.0            |1.0                      |0.8                 |1.0               |1.0                |1.0      |1.0              |BB+2                          |No Informado   |806487       |75029945.840000 |2026-10-20     |1             |806487       |1               |\n",
      "|809807       |0                 |8218     |,limit_rating_rga_esl,limit_risk_retention|0.0075              |1.0            |1.0                      |0.8                 |1.0               |1.0                |1.0      |1.0              |BBB2                          |BBB            |809807       |161245738.590000|2028-05-05     |1             |809807       |1               |\n",
      "|813447       |0                 |7803     |,limit_rating_rga_esl,limit_risk_retention|0.0075              |1.0            |1.0                      |0.8                 |1.0               |1.0                |1.0      |1.0              |BBB+1                         |A-             |813447       |50000000.000000 |2027-03-18     |1             |813447       |1               |\n",
      "|859007       |1                 |6713     |,limit_rating_rga_esl,limit_risk_retention|0.0075              |1.0            |1.0                      |0.8                 |1.0               |1.0                |1.0      |1.0              |BBB-2                         |No Informado   |859007       |46584929.840000 |2026-07-17     |1             |859007       |1               |\n",
      "|480045       |0                 |7803     |,limit_rating_rga_esl,limit_risk_retention|0.0075              |1.0            |1.0                      |0.8                 |1.0               |1.0                |1.0      |1.0              |BBB2                          |BBB            |480045       |82500000.000000 |2026-07-10     |1             |480045       |1               |\n",
      "+-------------+------------------+---------+------------------------------------------+--------------------+---------------+-------------------------+--------------------+------------------+-------------------+---------+-----------------+------------------------------+---------------+-------------+----------------+---------------+--------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "campos = list(dict_lim_ind.values())\n",
    "facilities_add.select(*key_facility,'limits_applied',*limits_indv,*campos).show(5,False)\n",
    "#.where(F.col('limit_rating_sp')<1).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d5c6d1d-45aa-4097-99ef-5abb702705b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST de los limites aplicados\n",
    "# n_filas = 5\n",
    "# for k,v in dict_lim_ind.items():\n",
    "#     l = 'limit_'+k\n",
    "#     print('limite testeado:',l)\n",
    "#     facilities_add.groupBy(l,v).count().orderBy(l,v).show(n_filas,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51ac5426-e97d-463c-91f1-cca5f90cf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating completo\n",
    "# facilities_add.groupBy('limit_rating_sp','g_lmscl_internal_ratg_type').count().orderBy('limit_rating_sp','g_lmscl_internal_ratg_type').show(50,False)\n",
    "\n",
    "# tipos de rating:\n",
    "# - g_lmscl_internal_ratg_type es el rating interno formato largo\n",
    "# - gf_pf_current_ratg_id es el de acreditadas (para project finance): viene marcado que sea este campo para titulizaciones project finance, es el de s&p\n",
    "# - group_rating_sp es el del grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6f6d57c-d93f-454b-8623-32d7d8a04f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(facilities_add.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14821420-f75b-4997-8e8c-b742507dfb3e",
   "metadata": {},
   "source": [
    "### Importe titulizable\n",
    "por cada facility, calculamos:\n",
    "- limite_individual: menor de sus limites, para quedarnos con el % m√°s restrictivo\n",
    "- importe titulizable = importe susceptible * limite_individual\n",
    "    -(*)para limites cuyo %vaya sobre el portofolio (rating_esg) se limita el importe susceptible al importe m√°ximo individual\n",
    "- candidata: facilities con importe titulizable\n",
    "marcamos si es excluida por limites:\n",
    "- 'excluded'=1, si algun limite 0 y son facilities excluidas por limites\n",
    "- 'exclusion_limit': el/los limites que son excluyentes porque su porcentaje marcado en el launchpad es 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcaaa55-ac67-44e8-a0ee-df28b8d778eb",
   "metadata": {},
   "source": [
    "#### Importe m√°ximo limite\n",
    "si es scope individual pero imp_limit portfolio, se trata a parte\n",
    "- viene marcado en el fichero limite-campo\n",
    "- imp_limit: sobre qu√© importe se calcula el % del limite (si es portfolio en un limite individual,\n",
    "    el % se calcula sobre el portofolio_size para ver si cumple el limite o no el importe titulizable de la facility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5490967a-0f63-4725-b1b8-91171c48704b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limite con importe sujeto a portfolio size: rating_rga_esl\n",
      "solo un limite individual con limite de importe portfolio\n"
     ]
    }
   ],
   "source": [
    "# identifico si hay limites individuales con imp_limit portfolio para tratarlos a parte (ya que su m√°ximo va sobre portfolio_size)\n",
    "l_pr = [l.limit_type for l in limites_ind.where(F.col('imp_limit')!='individual').select('limit_type').distinct().collect()]\n",
    "dict_lim_ind_p ={}\n",
    "facilities_add_p = facilities_add.withColumn('imp_maximo_individual', F.lit(portfolio_size)\n",
    "                                            ).withColumn('limit_individual_p',F.lit(1.0))\n",
    "\n",
    "# si existe limite con importe_maximo_individual\n",
    "if(len(l_pr)>0):\n",
    "    for k in l_pr:\n",
    "        print('limite con importe sujeto a portfolio size:', k)\n",
    "        dict_lim_ind_p[k]=dict_lim_ind[k]\n",
    "        if('limit_'+k in limits_indv):\n",
    "            limits_indv.remove('limit_'+k) # quitamos del c√°lculo del minimo limite individual\n",
    "        facilities_add_p = facilities_add_p.withColumn('imp_maximo_'+k, (F.col('limit_'+k)*portfolio_size).cast(\"float\"))#calculo ese importe m√°ximo\n",
    "        \n",
    "    if(len(l_pr)==1):\n",
    "        print('solo un limite individual con limite de importe portfolio')\n",
    "        facilities_add_p = facilities_add_p.withColumn('imp_maximo_individual', F.col('imp_maximo_'+k)\n",
    "                                                      ).withColumn('limit_individual_p',F.col('limit_'+k)) # el importe y limite m√°s restrictivo de los limites % sobre portfolio \n",
    "    else:\n",
    "        facilities_add_p = facilities_add_p.withColumn('imp_maximo_individual', F.least(*[F.col('imp_maximo_'+x) for x in l_pr])\n",
    "                                                      ).withColumn('limit_individual_p',F.least(*[F.col('limit_'+x) for x in l_pr])) # el importe y limite m√°s restrictivo de los limites % sobre portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dbed3cce-da9c-4e74-b43f-bca5a61c10cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o964.showString.\n: org.apache.spark.SparkException: Could not execute broadcast in 300 secs. You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:205)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:203)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareRelation(BroadcastHashJoinExec.scala:217)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:497)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:496)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:352)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:349)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:87)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:542)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:496)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:352)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:349)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:87)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:542)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:496)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:352)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:349)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:87)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:238)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:153)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:797)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.TimeoutException\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\t... 238 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LIMITES INDIVIDUALES comunes (el % va sobre el portfolio)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfacilities_add_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlimits_indv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/dist/python/lib/pyspark.zip/pyspark/sql/dataframe.py:486\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/dist/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/dist/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/dist/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o964.showString.\n: org.apache.spark.SparkException: Could not execute broadcast in 300 secs. You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:205)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:203)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareRelation(BroadcastHashJoinExec.scala:217)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:497)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:496)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:352)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:349)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:87)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:542)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:496)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:352)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:349)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:87)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:542)\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:496)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:352)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:349)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:87)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:238)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:153)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:346)\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:345)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:797)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.TimeoutException\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\t... 238 more\n"
     ]
    }
   ],
   "source": [
    "# LIMITES INDIVIDUALES comunes (el % va sobre el portfolio)\n",
    "facilities_add_p.select(*[F.col(x).cast('float') for x in limits_indv]).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7e554-1227-4abc-9081-8eeae488673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMITES INDIVIDUALES portfolio\n",
    "facilities_add_p.groupBy(*[F.col('limit_'+c)for c in l_pr],'imp_maximo_individual').count().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e127a-c593-459f-b1d9-3f31f802ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_tot = facilities_add_p.withColumn('limit_individual', F.least(*[F.col(x).cast('float') for x in limits_indv])\n",
    "                              ).withColumn('importe_titulizable_ini', F.col('importe_susceptible') * F.col('limit_individual')\n",
    "                              ).withColumn('importe_titulizable', F.when(F.col('importe_titulizable_ini')<=F.col('imp_maximo_individual'),F.col('importe_titulizable_ini')\n",
    "                                                                        ).otherwise(F.col('imp_maximo_individual'))\n",
    "                              ).withColumn('excluded', F.when(((F.col('limit_individual')==0)|(F.col('imp_maximo_individual')<=0)),1).otherwise(F.col('excluded'))\n",
    "                              ).withColumn('candidata',F.when(F.col('importe_titulizable')>0,1).otherwise(0))                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a026a-9d41-46ca-ae04-cfbd3581a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_tot.select(*key_facility,*limits_indv,'limit_individual','importe_susceptible','importe_titulizable_ini','importe_titulizable','imp_maximo_individual','excluded','exclusion_limit','candidata').show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af82f4a-6be1-4a3a-993d-0d9436d0a455",
   "metadata": {},
   "source": [
    "### A√±ado columna Motivo Exclusion\n",
    "para poder analizar el por q√∫e se excluye la facility\n",
    "- hay casos de importe susceptible en negativo, lo tomamos como importe 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b56d0f-5978-4ec9-9e94-585e0e40a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_tr = facilities_tot.withColumn('motivo_exclusion', F.when(F.col('excluded')==1,'limite individual 0'\n",
    "                                                         ).when(F.col('importe_susceptible')<=0,'importe susceptible 0'\n",
    "                                                         ).otherwise('NA')\n",
    "                              ).withColumn('detalle_exclusion', F.col('exclusion_limit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93d42d-02dc-4eca-9001-5fc908c2c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_tr.groupBy('candidata','motivo_exclusion').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0db5c-5742-4056-a7c6-d224a9e9f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobar caso an√≥malo\n",
    "# facilities_tr.where((F.col('candidata')==0)&(F.col('motivo_exclusion')=='NA')\n",
    "#                    ).select('limit_rating_rga_esl','importe_titulizable_ini','importe_titulizable','imp_maximo_individual').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bae39-725a-4fd4-a8ba-6aa8e4678fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facilities_tr.groupBy('candidata','motivo_exclusion').count().show()\n",
    "# +---------+--------------------+-----+\n",
    "# |candidata|    motivo_exclusion|count|\n",
    "# +---------+--------------------+-----+\n",
    "# |        0| limite individual 0|  254|\n",
    "# |        1|                  NA| 1207|\n",
    "# |        0|importe susceptib...|  155|\n",
    "# +---------+--------------------+-----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46de26a-269c-4c72-adf7-782642ebc7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facilities no excluidas por los limites individuales\n",
    "facilities_tr.where(F.col('excluded')==0).select(*key_facility,*limits_indv,'limit_individual',\n",
    "                                                 'importe_susceptible','importe_titulizable','imp_maximo_individual',\n",
    "                                                 'excluded','exclusion_limit').show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c83bf-8429-4eb4-8848-4411adbd7ed5",
   "metadata": {},
   "source": [
    "### Persistimos Sbx\n",
    "persistimos en sandbox las facilities excluidas de la cartera a titulizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ede33-0251-4356-9fca-012723939a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_facilities_lim_ind = root_pathc + '/facilities_limit_ind'\n",
    "path_facilities_lim_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5e8dd-fe4a-407c-9d5e-24b3cd7e895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_tr.write.parquet(path_facilities_lim_ind, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa5962-dd75-40a6-bf99-2abcc87f08a6",
   "metadata": {},
   "source": [
    "#### Estadisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a86018-51a7-41b3-b64d-856c364f7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataproc.read().parquet(path_facilities_lim_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342be53-a593-415f-b39c-b5559b21ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('facilities totales', df.count())\n",
    "print('facilities excluidas por limites individuales', df.where(F.col('excluded')==1).count())\n",
    "print('facilities con importe titulizable', df.where(F.col('candidata')==1).count()) # en la ultima titulizaci√≥n finalmente fueron 86 las facilities seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7b6e9-fa0e-42d8-b489-fa969dabb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pueden darse importes susceptibles negativos, pero se marcan como NO CANDIDATAS a entrar en la cartera\n",
    "# df.where(F.col('importe_titulizable')<0).show()\n",
    "# df.where(F.col('importe_titulizable')<0).select('candidata').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cf058-c8c2-4c93-bbbd-2a3f1940b581",
   "metadata": {},
   "source": [
    "#### Facilities excluded\n",
    "persistimos las facilities excluidas por limites individuales\n",
    "(*) ahora mismo en el visor, ver si seguimos dejando esta parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e708a-70a8-496d-b353-7355574553e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_excluded = root_pathc + '/facilities_excluded'\n",
    "path_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74c8aa-e672-47dc-a66c-b5e1922c02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_datio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db44c6a-0215-4bf3-8f6f-e5cb10f12b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_ex = set(key_facility + cols_datio)\n",
    "# cols_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66d83c-fc4b-40bf-814a-27144d9d0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_ex = cols_traza + key_facility + cols_datio\n",
    "df_exc = df.where(F.col('excluded')==1).select(*cols_traza,*cols_ex)\n",
    "df_exc.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa359f6-a8b0-4bf1-b5dc-f40ca9bf8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_exc.groupBy('customer_country').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882faa1-7494-45ec-b7ce-7fde95014a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(df_exc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3d27a-674b-4b6a-8690-8bd182ae3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exc.write.parquet(path_excluded, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c91cd-893f-4009-aac5-8bb9fe673b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# el expiration_date es menor a un a√±o respecto a la fecha de la titulizacion '2024-06-24'\n",
    "# df_exc.select('expiration_date','exclusion_limit').where(F.col('exclusion_limit').like('%maturity_min%')).show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406efd3-ee7d-4983-82fa-bbb7e24192a8",
   "metadata": {},
   "source": [
    "# 4. Limites Portfolio\n",
    "Una vez sabemos lo disponible de cada pr√©stamo, seleccionamos la cartera de los que pueden entrar\n",
    "- Limites Portfolio:ordenamos facilities y vamos consumiendo los l√≠mites globales\n",
    "- Resultado final: colectivo √≥ptimo de facilities a titulizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f828a3f-ff13-44b1-9bbb-f7ce85abc700",
   "metadata": {},
   "source": [
    "Marcan el % sobre el portfolio_size que se puede consumir de cada categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a98ae9-d00a-4cdf-9d1d-18cd09b7d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitamos lo limites marcados como cabecera (no interfieren en el ranking) y el sts\n",
    "# limit_cabecera = ['portfolio_size','portfolio_type','portfolio_date','sts','ccf','rw']\n",
    "# antes excluiamos los 0% porque estaban ya analizados .where(F.col('limit_value')!=0)\n",
    "\n",
    "limit_port = limites_total.where(F.col('limit_scope')=='portfolio'\n",
    "                            ).where(F.col('header_flag')==0\n",
    "                            ).where(F.col('limit_type')!='sts')\n",
    "\n",
    "                        # ).where(~(F.col('limit_type').isin(*limit_cabecera)))\n",
    "limit_port.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b263a8-fa47-44ce-93fc-5c6810e945ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('limites portfolio:', limit_port.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d42d39-e671-4bcc-a968-2c375a99bf82",
   "metadata": {},
   "source": [
    "## Diccionarios\n",
    "Generamos diccionarios con los datos de los limites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6e603-5bc2-4d53-bcd1-60e5810e5cac",
   "metadata": {},
   "source": [
    "### Diccionario limite-campo Datio\n",
    "correspondencia entre: limite - campo Datio\n",
    "- 'customer_subsector': 'g_asset_allocation_subsec_desc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7dac1a-238b-47f9-9140-35aa7b4e4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limites globales\n",
    "lista =limit_port.select('limit_type','campo_datio').collect()\n",
    "dict_lim_port = {row['limit_type']: row['campo_datio'] for row in lista}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9ab72-b38e-44f8-a36f-c1deeca05ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dict_lim_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1642c10-c0e3-49b7-93cc-bb4f9dba6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_portfolio = list(dict_lim_port.keys())\n",
    "# lim_portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab6ffb-2add-4318-a3ad-c5ee66a394cb",
   "metadata": {},
   "source": [
    "### Diccionario limite-valores\n",
    "Limites del launchpad: correspondencia entre: limite - categor√≠a - valor\n",
    "- 'customer_country-Spain': 0.3499999940395355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde54833-759a-4449-ad88-299847d2416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limites globales: no complejos\n",
    "lista1 =limit_port.select('limit_type','concept1_value','limit_value'\n",
    "                         ).where(F.col('complex_limit')==0).collect()\n",
    "dict_lim_values1 = {row['limit_type'] + '-' + row['concept1_value']:row['limit_value'] for row in lista1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810af68-a17b-44a1-ab0c-39dd2225c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(dict_lim_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ddd797-c6fc-400c-9d82-6a02a251ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limites globales: complejos\n",
    "lista2 =limit_port.select('limit_type','concept2_value','limit_value'\n",
    "                         ).where(F.col('complex_limit')==1).collect()\n",
    "dict_lim_values2 = {row['limit_type'] + '-' + row['concept2_value']:row['limit_value'] for row in lista2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687cf39-e269-4416-b59a-1c577a8f97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa992e-4e9d-427b-ab6f-f5abbe8c2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(dict_lim_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbacf4b9-1469-473b-abf1-915b6a77f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUSIONO en un √∫nico diccionario\n",
    "dict_lim_values= dict_lim_values1.copy()   # start with x's keys and values\n",
    "dict_lim_values.update(dict_lim_values2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7044ad-0aa3-4154-a5d5-2e08d102a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(dict_lim_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529be91-fbe4-4cd1-84f3-9f67dd1043b5",
   "metadata": {},
   "source": [
    "## Facilities disponibles\n",
    "- listado de facilities con traza de importe titulizable + limites individuales\n",
    "\n",
    "(*) cogemos todo el listado para tener traza de todo por si hay que revisar alg√∫n l√≠mite. S√≥lo entrar√≠an en la cartera a titulizar las candidatas = 1 (importe_titulizable>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6f629-75a8-44f6-a6c4-4bc37c878345",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_facilities_lim_ind = root_pathc + '/facilities_limit_ind'\n",
    "path_facilities_lim_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3298ed-2edc-4760-b525-5dc307fe7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pto de arranque son las facilities con importe titulizable\n",
    "facilities_disp = spark.read.parquet(path_facilities_lim_ind)\n",
    "print('facilities totales:',facilities_disp.count())\n",
    "print('facilities candidatas a titulizar:',facilities_disp.where(F.col('candidata')==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d10a5-5bf9-4d93-8725-d55e377a87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_disp.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5beb93f-747f-4b8d-ac7c-cc068c86ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facilities_disp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11de44-1f7d-40a6-b61b-ee5e5db66f6c",
   "metadata": {},
   "source": [
    "### Preparaci√≥n datos\n",
    "Transformados en Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52f354-9517-4757-8a72-1074be6d33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = facilities_disp.toPandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c0c17-87f7-4e86-a486-4f8582737510",
   "metadata": {
    "id": "dZ9DWvQLV3CF"
   },
   "source": [
    "#### Formateo de los datos\n",
    "Formatos correspondientes pyspark - pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5d22c-528d-4d71-aca3-45cfa0877477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipos al pasar a pandas\n",
    "# raw_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30d346-1e5d-41ab-bc91-11b52fadead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de tipologia PYSPARK de las columnas\n",
    "\n",
    "# listado de: columna - tipo de dato\n",
    "tipos = list(facilities_disp.dtypes)\n",
    "\n",
    "# en formato diccionario\n",
    "cols_type = dict(tipos)\n",
    "# cols_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c0cea-f3b2-4511-8fed-75d82131a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP PARA FORMATEAR PANDAS COLUMNAS DATIO DEL DICCIONARIO\n",
    "# posibles valores: 'string','date','decimal(,)','double','int','boolean'\n",
    "for r in raw_data.columns:\n",
    "    if (cols_type[r] == 'string'):\n",
    "        # print('string',r)\n",
    "        raw_data[r] = raw_data[r].astype('str')\n",
    "    elif (cols_type[r] == 'date'):\n",
    "        # raw_data[r] = raw_data[r].astype('datetime64')\n",
    "        raw_data[r] = raw_data[r].astype('datetime64[D]')\n",
    "        #print(r)\n",
    "    elif (cols_type[r] == 'boolean'):\n",
    "        raw_data[r] = raw_data[r].astype('bool')\n",
    "    elif (cols_type[r] == 'int'):\n",
    "        raw_data[r] = raw_data[r].astype('int')\n",
    "    elif (cols_type[r] == 'double'):\n",
    "        raw_data[r] = raw_data[r].astype('float')\n",
    "    elif ('decimal' in cols_type[r]):\n",
    "        raw_data[r] = raw_data[r].astype('float')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c4d80-ec2c-458e-9793-b7c28d35f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data.sort_values(by='importe_titulizable', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da131624-6611-4603-9a85-10cbde757dcc",
   "metadata": {
    "id": "dZ9DWvQLV3CF"
   },
   "source": [
    "#### Columnas limites portfolio\n",
    "- A√±adimos una columna para marcar la traza de los valores de limite portfolio que aplican por facility (por defecto 1.0)\n",
    "- selected = 1: si se ha seleccionado para la titulizacion (por defecto 0)\n",
    "- limit_portfolio = min(limites portfolio) que aplica a la facility (por defecto 1.0, un 100% de lo disponible) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1994de-01fc-43b5-a7df-52077ec904ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ini_columns():\n",
    "    cols_test=[]\n",
    "    \n",
    "    for k in lim_portfolio:\n",
    "        df['limit_'+k]=1.0000 # por defecto el limite es del 100%, en el proceso de cartera en los casos que hay limite marcado se actualiza\n",
    "        df['max_portfolio_size_'+k]= portfolio_size #por defecto, se puede coger el m√°ximo importe a titulizar\n",
    "        df['consumido_'+k]=0.0000 # % consumido para ese limite\n",
    "        df['importe_consumido_'+k]=0.0000 # importe consumido para ese limite\n",
    "        cols_test.append('limit_'+k)\n",
    "        cols_test.append('max_portfolio_size_'+k)\n",
    "        cols_test.append('consumido_'+k)\n",
    "        cols_test.append('importe_consumido_'+k)\n",
    "\n",
    "    df['selected']=0 # si se incluye en la cartera\n",
    "    df['limit_portfolio']=1.0000 # limite m√°s restrictivo a nivel portfolio\n",
    "    df['porcentaje_portfolio_size']= 0.0000 # % del importe del portfolio_size consumido por la facility (peso de la facility en la cartera)\n",
    "    df['importe_optimo']= 0.0000 # importe que se incluye de la facility\n",
    "    df['ranking_candidata']=0 # orden de prioridad en el modelo\n",
    "    df['ranking_selected']=0 # orden de selecci√≥n en la cartera final\n",
    "    df['importe_optimo_acumulado']=0.0000 # suma de los importes de las facilities incluidas en la cartera\n",
    "    df['porcentaje_portfolio_size_acumulado']=0.0000 # % de la cartera (portfolio_size) que se ha ido consumiendo\n",
    "    df['porcentaje_optimo']=0.0000 # % sobre el importe suceptible de la facility que se toma en la cartera\n",
    "    \n",
    "    # lo incluyo como columna el m√°ximo importe a titulizar\n",
    "    df['limit_portfolio_size']=portfolio_size\n",
    "\n",
    "    return cols_test + ['selected','limit_portfolio','porcentaje_portfolio_size','importe_optimo',\n",
    "                        'ranking_candidata','ranking_selected','importe_optimo_acumulado',\n",
    "                        'porcentaje_portfolio_size_acumulado','porcentaje_optimo','limit_portfolio_size']\n",
    "    \n",
    "cols_test = ini_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c480f42-1e78-4632-8eca-4ed3a5f42c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[*cols_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55269fa2-914f-482f-b2ef-d78096b8dc1d",
   "metadata": {
    "id": "dZ9DWvQLV3CF"
   },
   "source": [
    "### Consumos Limites\n",
    "- Por cada limites-categoria posible de las facilities:\n",
    "    - columna del consumo acumulado: Inicializamos a 0 consumo inicial\n",
    "    - columna del m√°ximo permitido: Calculamos el m√°ximo sobre el portfolio size (max_limite = portfolio_size * limite_launchpad)\n",
    "\n",
    "- Ej. posibles monedas: EUR,DOL,.. se ir√°n incrementando al seleccionar pr√©stamos con esa moneda y contrastando con su m√°ximo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0714d0-c391-4902-aead-335b808112a5",
   "metadata": {},
   "source": [
    "(*)Todos los posibles valores a nivel facility se tienen que corresponder con un limite,\n",
    "si no existe se genera por defecto con 1.0, que es el m√°ximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9642a-04b1-48f3-8612-bb50dde76940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_lim_port.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a6bb2-5c8a-485d-bdaa-ed5aaab0c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_lim_values['maturity_min-365'] # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aff3c9-3a6a-4bd0-82fb-ca4262ede762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(list(dict_lim_values.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1153b-4c50-47c6-821e-a09d15e82182",
   "metadata": {},
   "source": [
    "#### Diccionarios\n",
    "- con los consumos a 0\n",
    "- con el consumo_maximo calculado (portfolio_size*limite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709e471-7997-43c0-8bd4-6412f625cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_consumos():\n",
    "    l_lim_consumidos= {}# inicializamos diccionario de consumos\n",
    "    l_lim_marcados={} # inicializamos diccionario de limites\n",
    "    l_max_limites={} # calculo el m√°ximo portfolio por cada limite-categoria\n",
    "    limit_keys = list(dict_lim_values.keys()) # % marcados en launchad\n",
    "    keys_fechas = ['maturity_min','maturity_max']\n",
    "\n",
    "    for k,v in dict_lim_port.items():\n",
    "        # print(k,v)\n",
    "        if(k not in keys_fechas): # caso base: limite-categoria=valor      \n",
    "            for k1 in df[v].unique(): # recojo todos los posibles valores de ese campo en la facility\n",
    "                l_lim_consumidos[k+'-'+str(k1)]=0.0000\n",
    "\n",
    "                #si no hay marcado limite se pone a 1.0\n",
    "                if(k+'-'+str(k1) not in limit_keys):                 \n",
    "                    valor = 1.0000\n",
    "\n",
    "                    if(k+'-'+'total' in limit_keys): # si es un limite a nivel global como TODOS los grupos\n",
    "                        valor = round(float(dict_lim_values[k+'-'+'total']),4)\n",
    "\n",
    "                    dict_lim_values[k+'-'+str(k1)]=valor \n",
    "                    \n",
    "        else: # no es un limite directo y hay que calcular fechas: limite-ndias=valor\n",
    "            # print('limite:',k)\n",
    "            lk = [l for l in limit_keys if k in l][0]\n",
    "            dias = int(lk[len(k)+1:]) # ndias que marcan fecha tope\n",
    "            f_tope = np.datetime64(get_fecha(date_titulizacion,dias)).astype('datetime64[D]')\n",
    "            \n",
    "            for k1 in df[v].unique().astype('datetime64[D]'):\n",
    "                l_lim_consumidos[k+'-'+str(k1)]=0.0000                \n",
    "                # maturity_min: si la fecha maturity es mayor o igual no aplica limite\n",
    "                # maturity_max: si la fecha maturity es menor o igual no aplica limite\n",
    "                if(((k=='maturity_min')&(k1>=f_tope)) | ((k=='maturity_max')&(k1<=f_tope))):\n",
    "                    valor = 1.0000 # valor si el limite no aplica\n",
    "                else:\n",
    "                    valor = round(float(dict_lim_values[lk]),4) # valor si el limite aplica\n",
    "                    # print('fecha restringida:',k1, 'valor:',valor)\n",
    "                    \n",
    "                dict_lim_values[k+'-'+str(k1)]=valor\n",
    "\n",
    "\n",
    "    facilities_keys = list(l_lim_consumidos.keys()) \n",
    "    l_lim_marcados = {key: round(dict_lim_values[key],4) for key in facilities_keys}\n",
    "    l_max_limites = {key: (l_lim_marcados[key] * portfolio_size)  for key in l_lim_marcados}\n",
    "    return l_lim_consumidos,l_lim_marcados,l_max_limites\n",
    "\n",
    "\n",
    "    \n",
    "l_lim_consumidos,l_lim_marcados,l_max_limites= inicializar_consumos()\n",
    "# generamos un diccionario tb de importe consumido por cada limite\n",
    "l_importe_consumidos = l_lim_consumidos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a94c6-d6d1-40d9-a884-c7d2a857dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('listado de clave limites_categoria marcados en el lauchpad:',sorted(l_lim_marcados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddee69a-73e3-48e3-9c03-69fbeea1b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_l = list(l_lim_marcados)[0]\n",
    "key_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137eced-408c-4193-ae8f-9d7748eadcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lim_marcados[key_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7fe71-88f4-4108-b7bc-7e95c017fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lim_consumidos[key_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04969578-ab76-4f05-ad4c-432080bdf093",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_max_limites[key_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674026a8-3350-4c06-871e-20cf175fb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lim_marcados['customer_country-Austria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860deb4-2662-434e-a18d-fd1df2ebeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_max_limites['customer_country-Austria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da334e89-df29-4d5e-af82-8823ae67f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lim_marcados['sts_group-G00000000000005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eaab16-037b-4a2b-82da-6ab4db1195ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_max_limites['sts_group-G00000000000005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594d76b-2ce6-497e-945a-08e2fd3a507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lim_marcados['group-G20080109113719']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645164d-0e1c-4c20-8761-4516400b6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_max_limites['group-G20080109113719']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44910461-3a4f-41a4-901f-b455072515f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_lim_values['customer_country-Brazil']\n",
    "# l_lim_marcados['customer_country-Brazil']\n",
    "# l_lim_marcados['maturity_min-2025-02-05']\n",
    "# l_lim_marcados['maturity_min-2026-10-13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365d91d-dc71-4be8-9652-15b8f7fdba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(l_lim_marcados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be05eb4-ba42-4c30-9324-6625cb69d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_lim_consumidos\n",
    "\n",
    "# 'financial_product': {'Term Loan': 0,\n",
    "#   'RCF': 0,\n",
    "#   'Credit Line and Guarantee': 0,\n",
    "#   'Guarantee': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6c12e-67ca-4af6-9b8a-7c8a7759ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUEBA para limites en concreto\n",
    "# divisa_importe = {divisa: 0 for divisa in df['currency_id'].unique()} # posibles valores de divisas en nuestras facilities\n",
    "# pais_importe = {pais: 0 for pais in df['customer_country'].unique()} # posibles valores de paises en nuestras facilities\n",
    "\n",
    "# divisa_importe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efccf7f-bb0e-44be-a440-917b90667227",
   "metadata": {},
   "source": [
    "## Generaci√≥n Cartera\n",
    "Nos quedamos con las facilities que entran en la titulizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322f89f-cc0e-48e8-9b1c-3ad8d6b94c9c",
   "metadata": {},
   "source": [
    "### Limites por facility\n",
    "Funci√≥n para obtener campo-valor de la facility a contrastar con los limites marcados\n",
    "- 'customer_country-United States of America'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80616f19-a094-4b3d-9b7e-b407ba400710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos en un diccionario columna Datio - valor para la facility analizada\n",
    "def get_keys_facility(i_facility, dataframe):   \n",
    "    d_keys={} # formato lista de clave-valor\n",
    "    res={} # diccionario de campos\n",
    "    \n",
    "    # para cada limite, el valor del campo de Datio indicado \n",
    "    # ej. para facility fila 1 -> divisa = df.loc[1, 'currency_id'] -> 'divisa':'USD'\n",
    "    for k,v in dict_lim_port.items():    \n",
    "        res[k]=dataframe.loc[i_facility, v]\n",
    "        \n",
    "        if (cols_type[v] == 'date'):\n",
    "            res[k]=dataframe.loc[i_facility, v].strftime('%Y-%m-%d')\n",
    "        \n",
    "    for k,v in res.items():\n",
    "        key = str(k) + '-' + str(v)\n",
    "        d_keys[k]=key\n",
    "        \n",
    "    return d_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecaabc-1cee-4c89-804c-408fa7b732b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lim_port.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417ab3b-1d8c-4d83-8556-d081b8f5d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_keys_facility(0,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09288b-239b-4a3f-a62d-0ba703f0ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_keys_facility(0, df)\n",
    "\n",
    "# {'customer_subsector': 'customer_subsector-Integrated Utility',\n",
    "#  'customer_country': 'customer_country-United States of America',\n",
    "#  'customer_sector': 'customer_sector-Utilities',\n",
    "#  'financial_product': 'financial_product-Term Loan',\n",
    "#  'non_ig': 'non_ig-0',\n",
    "#  'sts_group': 'sts_group-G00000000010863',\n",
    "#  'divisa': 'divisa-USD',\n",
    "#  'group': 'group-G00000000010863'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59865979-2b0c-40b1-b7ed-7c2f5d4fc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_keys_facility(54, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917faaa-ae4a-4dcb-9b47-0ab1febba706",
   "metadata": {},
   "source": [
    "**dudas de como aplicar el limite rw (usa para ponderar la ordenaci√≥n por riesgo y es 0.9 sobre el RORC)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf09ce9-c758-4e69-9c35-eaf1e03d6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnas candidatas sobre las que aplican los %\n",
    "# cols_ajuste = ['Total_Amount_EUR','Total_Amount_CCY','EC_per','RC_per','Risk_Weight','EL_per','Reg_EL_per']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98f4bd-bf46-4b33-814c-b8e627785bd0",
   "metadata": {
    "id": "dZ9DWvQLV3CF"
   },
   "source": [
    "### Ordenaci√≥n Facilities\n",
    "- Ordenamos de mayor a menor importe a titulizar\n",
    "\n",
    "(*) segunda versi√≥n incluir tb ordenar de mayor a menor riesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d91f78-75db-4ee0-9f88-6e7f3e5fd365",
   "metadata": {
    "id": "AdLRpgzsko_B"
   },
   "outputs": [],
   "source": [
    "# columnas de ordenacion: importe titulizable del pr√©stamo. segunda fase probar tb a incluir el porcentaje de riesgo\n",
    "cols_ord = ['importe_titulizable']#, 'RC_per']\n",
    "\n",
    "df = df.sort_values(by=cols_ord, ascending=False).reset_index(drop=True)\n",
    "\n",
    "# ** parte inicialmente no requerida ***#\n",
    "# HACERLO PARA TODOS LOS CAMPOS NECESARIOS EN LOS LIMITES (con el diccionario de limites)\n",
    "# Convertir datos a listas para su uso en Pulp y OR-Tools\n",
    "# importes = data['Total_Amount_EUR'].tolist()\n",
    "# riesgos = data['RC_per'].tolist()\n",
    "# divisas = data['currency_id'].tolist()\n",
    "# paises = data['customer_country'].tolist()\n",
    "# ranking = data.index.values.tolist()\n",
    "# n = len(importes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c208d2f-1bff-42a5-820b-32ca079d5247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[[*key_facility,*cols_ord,'candidata']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1890a-82ca-4120-bb81-e317e93fdb36",
   "metadata": {},
   "source": [
    "### Inicializaci√≥n de variables\n",
    "Inicializamos las variables necesarias en el modelo de selecci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b610f-7f20-4454-8427-6e712d7e1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el contador del importe acumulado de las facilities\n",
    "importe_acumulado = 0.0000\n",
    "# Inicializamos el % carterizado sobre el total a titulizar\n",
    "porcentaje_portfolio_size_acumulado = 0.0000000\n",
    "\n",
    "# marcamos el m√°ximo a titulizar, que es el importe que se quiere titulizar\n",
    "maximo_importe = portfolio_size\n",
    "\n",
    "# columna sobre el importe de la facility disponible a ser titulizado\n",
    "col_imp = 'importe_titulizable'\n",
    "\n",
    "# Lista para almacenar los √≠ndices de las facilities seleccionadas y sus proporciones\n",
    "selected_facilities = []\n",
    "\n",
    "# los contadores de limites\n",
    "# l_lim_consumidos: todos a 0 inicialmente, l_lim_marcados: % marcados en el launchad, l_max_limites: portfolio_size por limite\n",
    "l_lim_consumidos,l_lim_marcados,l_max_limites = inicializar_consumos() # inicializamos los consumos antes de comenzar\n",
    "\n",
    "# columnas a√±adidas en el proceso\n",
    "cols_test = ini_columns() # inicializamos los valores de las columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558b624-22ca-44c3-b774-cda7769cb1e5",
   "metadata": {},
   "source": [
    "### Modelo de titulizacion\n",
    "Proceso para generar la cartera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773237fa-a550-4527-92d1-0549bc03297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si queremos ver el detalle de la traza de ciertas facilities\n",
    "traza_candidata = []#['830990','815808','848947']#[]#['185065','783009']\n",
    "traza_exclusion = True # para almacenar traza en la cartera final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b968d-5eb8-4247-9a5d-48837620e6bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "4lUuZUgvimaA",
    "outputId": "968802b8-7d2e-4f63-826e-44d7fb510e89"
   },
   "outputs": [],
   "source": [
    "for i in df.index: # para recoger cada numero de fila (1 fila por facility)\n",
    "    if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "        print('**** facility ',i,'****************************************************')\n",
    "        print('---- id_facilitiy',df.loc[i]['delta_file_id'])\n",
    "    \n",
    "    # PASO_1: importe titulizable de la facility\n",
    "    expediente_importe = df.loc[i, col_imp]\n",
    "    df.loc[i,'ranking_candidata']=i+1 # posicion en el ranking de la facility dentro de las candidatas en la cartera (iniciamos en 1)\n",
    "    \n",
    "    if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "        print('expediente_importe',expediente_importe)\n",
    "    \n",
    "    \n",
    "    # PASO_2: %m√°ximo a titulizar de la facility    \n",
    "    # inicializamos el m√°x titulizable de la facility\n",
    "    max_proportion = 1.0000 # por defecto se intenta titulizar todo lo disponible por facility\n",
    "    \n",
    "    # si el importe de la facility ya excede lo que hay disponible para titulizar se calcula ese m√°ximo\n",
    "    if((importe_acumulado + expediente_importe) > maximo_importe):\n",
    "        max_proportion = (maximo_importe - importe_acumulado) / expediente_importe # max % que se puede titulizar de la facility\n",
    "    \n",
    "    if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "        print('maximo:',max_proportion)\n",
    "        \n",
    "    \n",
    "    # PASO_3: %m√≠nimo marcado por los limites que ser√° lo m√°ximo que se puede titulizar\n",
    "    # campos de la facility - valor que hay que limitar\n",
    "    keys_f = get_keys_facility(i,df)\n",
    "    if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "        print(keys_f)\n",
    "    min_lim= max_proportion # inicializamos el m√≠nimo con el m√°ximo posible a titulizar\n",
    "    min_portfolio=max_proportion\n",
    "    for k,v in keys_f.items():\n",
    "        #print('limite aplicar:',l_lim_marcados[v])\n",
    "        df.loc[i, 'limit_'+k] = l_lim_marcados[v] # genero el limite del campo y actualizo valor para la categor√≠a de esa facility\n",
    "        df.loc[i,'max_portfolio_size_'+k]= l_max_limites[v]\n",
    "        min_portfolio = min(min_portfolio,l_lim_marcados[v])# minimo de los limites de portfolio       \n",
    "        disponible_proportion = round(float(l_lim_marcados[v] - l_lim_consumidos[v]),4)\n",
    "        #***********************\n",
    "        if(disponible_proportion<0): # debido al redondeo de decimales\n",
    "            disponible_proportion = 0.0000\n",
    "            # print('limit_'+k,l_lim_marcados[v])\n",
    "            # print('consumido',l_lim_consumidos[v])\n",
    "            # print(i)\n",
    "        #***********************\n",
    "        df.loc[i, 'disponible_'+k] = disponible_proportion\n",
    "        min_lim = min(min_lim,disponible_proportion) # minimo que se puede aplicar teniendo en cuenta lo que ya est√° consumido   \n",
    "        if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "            print('limite-categoria',v,'% m√°ximo',min_lim, 'marcado',l_lim_marcados[v],'consumido',l_lim_consumidos[v], 'disponible',disponible_proportion)\n",
    "            \n",
    "    if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "        print(' proporcion m√°xima:',min_lim)\n",
    "    df.loc[i, 'limit_portfolio'] = min_portfolio\n",
    "    df.loc[i, 'porcentaje_max_portfolio'] = min_lim # m√°ximo % disponible de los consumos de los limites\n",
    "    \n",
    "    # PASO_4: Comprobamos si esa facility se puede incluir y sino pasamos a la siguiente\n",
    "    # que sea candidato por importe titulizable>0 y que no se haya consumido alguno de sus limites\n",
    "    if((df.loc[i, 'candidata']==1) and (min_lim>0)):\n",
    "       \n",
    "        # PASO_5: Calculamos el importe a titulizar de la facility\n",
    "        importe_max_facility = maximo_importe * min_lim # importe m√°ximo a titulizar para la facility: portfolio_size*%m√°s restrictivo\n",
    "        df.loc[i,'importe_max_facility'] = importe_max_facility # guardamos el importe m√°ximo que se puede coger de esa facility\n",
    "        # si el importe que se puede incluir es mayor o igual al de la facility incluimos todo\n",
    "        if(importe_max_facility >= expediente_importe):\n",
    "            importe_seleccionado = expediente_importe\n",
    "        else:\n",
    "            importe_seleccionado = importe_max_facility\n",
    "        if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "            print('importe_seleccionado:',importe_seleccionado)\n",
    "\n",
    "         # Si el importe √≥ptimo>0, \n",
    "        if(importe_seleccionado>0):\n",
    "            # PASO_6: Actualizamos los acumulado tanto de limites como de importe a titulizar\n",
    "            importe_acumulado = importe_acumulado + importe_seleccionado\n",
    "            por_consumido = round(float(importe_seleccionado/maximo_importe),7) # peso_cartera: % de la titulizacion sobre la cartera\n",
    "            if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "                print('% usado de la cartera (porcentaje_portfolio_size):',por_consumido)\n",
    "                \n",
    "            for k,v in keys_f.items():\n",
    "                l_lim_consumidos[v] = round(float(l_lim_consumidos[v] + por_consumido),7) # actualizo los consumos limite-categoria\n",
    "                l_importe_consumidos[v] = l_importe_consumidos[v] + importe_seleccionado\n",
    "                df.loc[i, 'consumido_'+k] = l_lim_consumidos[v]\n",
    "                df.loc[i, 'importe_consumido_'+k] = l_importe_consumidos[v]\n",
    "                if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "                    print('consumido ',v,':',l_lim_consumidos[v])\n",
    "                    print('importe_consumido ',v,':',l_importe_consumidos[v])\n",
    "\n",
    "            # PASO_7: Rellenamos traza a nivel facility\n",
    "            # Marcamos la facility como seleccionada con su proporcion_optima y su importe_optimo\n",
    "            selected_facilities.append((i, por_consumido))  \n",
    "            df.loc[i,'selected']=1\n",
    "            df.loc[i,'importe_optimo'] = importe_seleccionado\n",
    "            df.loc[i,'porcentaje_optimo']= round(float(importe_seleccionado/df.loc[i,'importe_susceptible']),7) # % del importe susceptible que finalmente se coge\n",
    "            df.loc[i,'ranking_selected']=len(selected_facilities) # posicion en el ranking de las facilities seleccionadas\n",
    "            df.loc[i,'importe_optimo_acumulado']= importe_acumulado\n",
    "            porcentaje_portfolio_size_acumulado = round(float(importe_acumulado/maximo_importe),7) # % usado del portfolio_size\n",
    "            df.loc[i,'porcentaje_portfolio_size_acumulado']= porcentaje_portfolio_size_acumulado\n",
    "            df.loc[i, 'porcentaje_portfolio_size'] = por_consumido # porcentaje del portfolio_size que se consume (% sobre la cartera final)\n",
    "            \n",
    "            # TRAZA DE SALIDA\n",
    "            if(df.loc[i]['delta_file_id'] in traza_candidata):\n",
    "                print('---- facility seleccionada:',df.loc[i,'ranking_selected'])\n",
    "                min_disponible=1.0\n",
    "                print('***Se ha alcanzado un limite a nivel columna-categoria. NUM CANDIDATA: ***',i+1)\n",
    "                for k,v in keys_f.items():\n",
    "                    print('----- limite analizado:',v)\n",
    "                    print('limite en launchpad:',l_lim_marcados[v])\n",
    "                    print('consumido:',l_lim_consumidos[v])      \n",
    "                    disponible_proportion = round(float(l_lim_marcados[v] - l_lim_consumidos[v]),4)\n",
    "                    if(disponible_proportion<0): # debido al redondeo de decimales\n",
    "                        disponible_proportion = 0.0000\n",
    "                    print('disponible:',disponible_proportion)\n",
    "                    min_disponible = min(min_disponible,disponible_proportion)  \n",
    "                print('m√°ximo disponible para la facility:',min_disponible)\n",
    "                \n",
    "    # para aquellas facilities que no entran dejamos traza y arrastran los consumos e importes acumulados hasta el momento\n",
    "    else:\n",
    "        if(df.loc[i, 'candidata']==1): #solo para aquellas candidatas (que tengan importe disponible)\n",
    "            min_disponible=1.0\n",
    "            print('***Se ha alcanzado un limite a nivel columna-categoria. NUM CANDIDATA: ',i+1, 'DELTA FILE:',df.loc[i]['delta_file_id'],'***')\n",
    "            for k,v in keys_f.items():    \n",
    "                disponible_proportion = round(float(l_lim_marcados[v] - l_lim_consumidos[v]),4)\n",
    "                if(disponible_proportion<0): # debido al redondeo de decimales\n",
    "                    disponible_proportion = 0.0000\n",
    "                if(disponible_proportion==0):\n",
    "                    print('----- limite analizado:',v)\n",
    "                    print('limite en launchpad:',l_lim_marcados[v])\n",
    "                    print('consumido:',l_lim_consumidos[v]) \n",
    "                    print('disponible:',disponible_proportion)\n",
    "                    min_disponible = min(min_disponible,disponible_proportion)\n",
    "                    if(l_lim_marcados[v]==0):\n",
    "                        df.loc[i, 'motivo_exclusion']='limite portfolio 0'\n",
    "                        df.loc[i,'detalle_exclusion']=v\n",
    "                    else:\n",
    "                        df.loc[i, 'motivo_exclusion']='consumido limite portfolio'\n",
    "                        df.loc[i,'detalle_exclusion']=v                            \n",
    "            print('m√°ximo disponible para la facility:',min_disponible)\n",
    "            \n",
    "        # arrastramos valores acumulados\n",
    "        df.loc[i,'importe_optimo_acumulado']= importe_acumulado # importe acumulado hasta el momento\n",
    "        df.loc[i,'porcentaje_portfolio_size_acumulado']= porcentaje_portfolio_size_acumulado # % usado del portfolio_size hasta el momento\n",
    "        for k,v in keys_f.items():\n",
    "            df.loc[i, 'consumido_'+k] = l_lim_consumidos[v]\n",
    "            df.loc[i, 'importe_consumido_'+k] = l_importe_consumidos[v]\n",
    "        \n",
    "    \n",
    "    # PASO_8: si se ha alcanzado el m√°ximo a titulizar salimos del bucle, ya tenemos las facilities a titiulizar\n",
    "    if(importe_acumulado>=maximo_importe):\n",
    "        print('***Se ha superado el portfolio size. Importe acumulado:',importe_acumulado)\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69703a68-e112-45f8-a56a-df6a46db6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[*cols_test,'importe_titulizable','porcentaje_max_portfolio','porcentaje_optimo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62b002-b3d1-4624-afcd-353386a7b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df['selected']==1).sum()\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96659e64-5931-4768-85b9-a42faadcbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_lim_marcados['customer_country-United States of America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e52bc3-e4f4-4a75-8841-26e05a3f1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_lim_consumidos['customer_country-United States of America']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055e922-23f1-4e35-bf83-8bdf4e29e593",
   "metadata": {},
   "source": [
    "# 5. Persistimos\n",
    "- Pasamos de pandas a pyspark para guardar el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74776653-eb6a-4e71-aa15-231ead7ac9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = dataproc.getSparkSession().createDataFrame(df).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf527cb-c4c1-428d-ad7c-53b498fa3939",
   "metadata": {},
   "source": [
    "## Ajustes de tipos Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2ab26-1d45-4730-a4ee-4b9e170947a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos = []\n",
    "for c in df_sp.columns:\n",
    "    tipo = df_sp.schema[c].dataType\n",
    "    # print(c,tipo)\n",
    "    if tipo not in tipos:\n",
    "        tipos.append(tipo)\n",
    "        \n",
    "    # redondeo los tipos num√©ricos a 2 decimales    \n",
    "    if str(tipo) in ('LongType','DoubleType'):\n",
    "        # print(c)\n",
    "        if(c in ['porcentaje_portfolio_size','porcentaje_portfolio_size_acumulado']):\n",
    "            df_sp = df_sp.withColumn(c, F.round(F.col(c),7)) #dejamos un redondeo de 6 decimales \n",
    "        else:\n",
    "            df_sp = df_sp.withColumn(c, F.round(F.col(c),4)) #dejamos un redondeo de 4 decimales \n",
    "                                       \n",
    "print('tipo de datos de las columnas:',tipos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c661f1-cf9a-4470-8df8-64acf9943514",
   "metadata": {},
   "source": [
    "### parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528fb1a-3ffd-4a2c-8bff-2798e393f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = root_path+'closing_date='+str(fecha)+'/cartera_titulizar'\n",
    "path_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6b6cd-2524-4397-b327-a8789a89eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp.write.parquet(path_out,mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736654fa-d142-4b55-a829-dab50f10c861",
   "metadata": {},
   "source": [
    "### csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9b4ca-cc4e-425e-9f2a-aa6a99b29b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out_csv = root_path+'closing_date='+str(fecha)+'/cartera_titulizar_csv'\n",
    "path_out_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9aee02-69f9-45bf-a786-332e1d7916f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataproc.read().parquet(path_out).coalesce(1).write.csv(path_out_csv, mode='overwrite', header='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3cdab-2440-4c10-8a27-b4df79088e7b",
   "metadata": {},
   "source": [
    "# 6. Estad√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef644d-7675-4b79-9ba9-1a32fd1c25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_ejecucion = last_partition (root_path, 'closing_date')\n",
    "print('Fecha de ejecuci√≥n del modelo titulizaci√≥n:', fecha_ejecucion)\n",
    "root_pathc = root_path + 'closing_date=' + str(fecha_ejecucion)\n",
    "root_pathc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252ce14-e3a9-457b-b5a4-bef87a95d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = root_pathc+'/cartera_titulizar'\n",
    "path_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae0f47-f8eb-4543-985b-cdf77960d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = dataproc.read().parquet(path_out)\n",
    "cartera_candidatas = df_total.where(F.col('candidata')==1)\n",
    "cartera_titulizar = df_total.where(F.col('selected')==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5930a2-e95b-4218-91d0-46f9b27405a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571143c-be78-49ab-bdb4-a0ae51200f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_t = ['delta_file_id','delta_file_band_id']\n",
    "n= df_total.groupBy(*key_t).agg(F.count('delta_file_id').alias('n')).where(F.col('n')>1).count()\n",
    "if (n>1):\n",
    "    print('MicroStrategy: Hay duplicados a nivel expediente-tramo')\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e69df-848d-4478-a37d-ecd017f716c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_delta = ['185065','783009']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af840b9-afa5-420a-9029-3c0b470e00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.where(F.col('delta_file_id').isin(ids_delta)).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dcb5ca-5406-44d3-8581-ebd99b04ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_t = ['delta_file_id','delta_file_band_id','branch_id']\n",
    "if (df_total.groupBy(*key_t).agg(F.count('delta_file_id').alias('n')).where(F.col('n')>1).count()>1):\n",
    "    print('Hay duplicados a nivel facility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fddb8-c56e-43e3-878c-204b7cb7b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30073e6d-b498-4be0-ad28-670d6aaf590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(df_total.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d641ea-efc2-4644-a8ca-3f968279f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('numero total de facilities', df_total.count())\n",
    "print('numero de facilities candidatas a titulizar', cartera_candidatas.count())\n",
    "print('numero de facilities en la cartera a titulizar', cartera_titulizar.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e06cd-e3b5-41bc-9c5c-26bcdf98dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importes = df_total.groupBy().agg(F.sum('importe_susceptible').alias('importe_susceptible'),\n",
    "                 F.sum('importe_titulizable').alias('importe_titulizable'),\n",
    "                 F.sum('importe_optimo').alias('importe_final_titulizar'),\n",
    "                 F.collect_set('limit_portfolio_size')[0].alias('portfolio_size'))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13bbfc0-6d4c-4992-bfb6-b917f0d14bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "suma_sus,suma_imp_d,suma_imp,importe_titulizar = [(x.importe_susceptible,x.importe_titulizable,x.importe_final_titulizar,x.portfolio_size) for x in df_importes.collect()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91cb2ba-d933-42a3-bab4-4ef5512cf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suma_imp_d = [x.importe_titulizable for x in cartera_candidatas.groupBy().agg(F.sum('importe_titulizable').alias('importe_titulizable')).collect()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7cb4f-e067-4b03-b712-164002d16fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suma_imp = [x.importe_final_titulizar for x in cartera_titulizar.groupBy().agg(F.sum('importe_optimo').alias('importe_final_titulizar')).collect()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d138a1c8-ac2a-4be6-8757-78b3ed9db5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importe_titulizar = [x.limit_portfolio_size for x in df_total.select('limit_portfolio_size').distinct().collect()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40354f-df99-4d0a-b86f-03edbecf143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('importe que se quiere titulizar',importe_titulizar) # 2.000.000.000\n",
    "print('importe susceptible a titulizar',suma_sus) # 63.917.592.977\n",
    "print('importe total titulizable', suma_imp_d) # 3.539.030.085\n",
    "print('importe final titulizado', suma_imp)  # 793.488.411\n",
    "print('porcentaje titulizado del total a titulizar', str(suma_imp/importe_titulizar)) # 0.39\n",
    "print('se deja sin titulizar', str(importe_titulizar - suma_imp)) # 1.206.511.588"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87700ee5-70cc-4c72-aa9a-6ae4cd8f54f8",
   "metadata": {},
   "source": [
    "### Trazas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa100aa-7b4d-46a9-9f7f-a1ecfef2663f",
   "metadata": {},
   "source": [
    "#### importe susceptible\n",
    "- importe1 = saldo vivo+importe disponible*CCF\n",
    "- importe2 = importe_titulizado/risk_retention\n",
    "- importe_susceptible = min(ead_regulatorio,importe1) - importe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e099f-d026-4466-9e79-fe57c09457c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_cal = ['bbva_drawn_eur_amount','bbva_available_eur_amount','limit_ccf','importe1',\n",
    "          'gf_facility_securitization_amount','limit_risk_retention','importe2',\n",
    "          'gf_ma_ead_amount','importe_susceptible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864adcd-6b30-40e3-a0e9-8c6dfa0c41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.select(*key_limites,*key_facility,*c_cal).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a9e75-e85b-48af-aa2f-de49ceab04d3",
   "metadata": {},
   "source": [
    "#### limites individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f25f9c-b22b-4f6d-a621-5cbad6c7f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_lim_ind = ['limit_rating_sp','limit_excluded_facilities','limit_risk_retention',\n",
    "             'limit_sts_rw_modelo','limit_sts_payment','limit_individual']\n",
    "c_imp = ['importe_susceptible','importe_titulizable']\n",
    "c_ex = ['excluded','exclusion_limit']\n",
    "c_flag_i = ['candidata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313406a-f311-4344-8cbd-f47220038d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.select(*key_limites,*key_facility,*c_lim_ind,*c_imp,*c_ex,*c_flag_i).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36b87b-2fe9-46d5-b356-a40938e89531",
   "metadata": {},
   "source": [
    "#### limites portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8e086-ace0-4f51-adad-71a604a63b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_flag_p = ['selected','ranking_candidata','ranking_selected']\n",
    "c_lim_portfolio =['limit_customer_subsector','consumido_customer_subsector',\n",
    "                  'limit_customer_country','consumido_customer_country',\n",
    "                  'limit_customer_sector','consumido_customer_sector',\n",
    "                  'limit_financial_product','consumido_financial_product',\n",
    "                  'limit_non_ig','consumido_non_ig',\n",
    "                  'limit_sts_group','consumido_sts_group',\n",
    "                  'limit_divisa','consumido_divisa',\n",
    "                  'limit_group','consumido_group',\n",
    "                  'limit_portfolio']\n",
    "c_imp_p =['importe_titulizable','porcentaje_portfolio_size','importe_optimo',\n",
    "         'porcentaje_portfolio_size_acumulado','importe_optimo_acumulado']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e99e5-e21f-4583-846a-cbb15f1eacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.where(F.col('candidata')==1 # para analizar las facilities que son candidatas en la titulizacion \n",
    "              ).select(*key_limites,*key_facility,*c_lim_portfolio,*c_imp_p,*c_flag_p).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff8170-f2df-43dc-b729-493ddc95461c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402ced8-f32e-47ec-94aa-5c245055d7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925b2f9-d0c5-4219-b53f-3dc55c855d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-python",
   "language": "python",
   "name": "spark-python-spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
